import os
import torch
import numpy as np
import random
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import cv2
from skimage.feature import local_binary_pattern
import torchvision
import torchvision.transforms as T
from tqdm import tqdm
from sklearn.metrics import f1_score, confusion_matrix
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc , accuracy_score
from prettytable import PrettyTable
from collections import defaultdict, Counter
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import label_binarize
import pandas as pd
from sklearn.utils import resample
import seaborn as sns
plt.rcParams.update({
    'font.size': 12,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 11,
    'ytick.labelsize': 11,
    'legend.fontsize': 11,
    'axes.linewidth': 0.8,      # ç»†è¾¹æ¡†
    'lines.linewidth': 1.1,     # ç»†çº¿
    'axes.grid': False,         # å…³é—­èƒŒæ™¯grid
    'savefig.transparent': True # å¯¼å‡ºé€æ˜èƒŒæ™¯
})
sns.set_style('white')
custom_palette = ["#4C72B0", "#55A868", "#C44E52", "#8172B3", "#CCB974", "#64B5CD"]
sns.set_palette(custom_palette)
from torch.utils.data import TensorDataset
import json
from sklearn.metrics import precision_score, recall_score

# ========== 1. Utility Functions ==========
def set_seed(seed=42):
    """Set all random seeds for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False



def evaluate_dynamic_fusion(fusion_model, test_logits, test_labels, device, config):
    """è¯„ä¼°Dynamic Weight Fusionæ¨¡å‹"""

    fusion_model.eval()
    test_dataset = TensorDataset(test_logits, test_labels)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for logits_batch, labels_batch in test_loader:
            logits_batch = logits_batch.to(device)

            fused_output, weights = fusion_model(logits_batch)
            probs = F.softmax(fused_output, dim=1)
            preds = fused_output.argmax(dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels_batch.numpy())
            all_probs.extend(probs.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    metrics = calculate_unified_metrics(all_labels, all_preds, all_probs, config['class_names'])

    # æ‰“å°ç»“æœ
    print(f"\n{'=' * 20} Dynamic Fusionæµ‹è¯•ç»“æœ {'=' * 20}")
    print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    print(f"F1åˆ†æ•° (Macro): {metrics['f1_macro']:.4f}")
    print(f"AUC: {metrics['auc']:.4f}")

    return {
        'metrics': metrics,
        'predictions': (all_labels, all_preds, all_probs)
    }


def bootstrap_metric(y_true, y_pred, y_prob, metric_func, n_iterations=500, ci=0.95):
    scores = []
    n_samples = len(y_true)

    for _ in range(n_iterations):
        indices = resample(range(n_samples), replace=True, n_samples=n_samples)
        score = metric_func(
            y_true[indices],
            y_pred[indices],
            y_prob[indices] if y_prob is not None else None
        )
        scores.append(score)

    alpha = (1 - ci) / 2
    lower = np.percentile(scores, 100 * alpha)
    upper = np.percentile(scores, 100 * (1 - alpha))
    return np.mean(scores), (lower, upper)

def extract_serial_number(folder_name):
    """Extract serial number from folder name"""
    import re
    match = re.match(r'^(\d+)', folder_name)
    if match:
        return int(match.group(1))
    return None


def extract_yellow_metrics(eye_patch):
    """Extract jaundice-specific color features from eye patches"""
    img = np.array(eye_patch)
    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)

    # Yellow detection in multiple color spaces
    yellow_hsv = cv2.inRange(hsv, np.array([20, 50, 50]), np.array([40, 255, 255]))

    # Extract b* channel from LAB (yellow-blue axis)
    b_channel = lab[:, :, 2]
    yellow_lab = b_channel > 128  # High b* values indicate yellow

    # Combine metrics
    features = [
        np.mean(yellow_hsv > 0),  # Yellow percentage in HSV
        np.mean(yellow_lab),  # Yellow percentage in LAB
        np.std(b_channel),  # Variation in yellowness
        np.percentile(b_channel, 90)  # Top 10% yellowness
    ]

    return np.array(features)


def plot_multiclass_roc(y_true, y_score, class_names, save_path=None):
    import seaborn as sns
    n_classes = len(class_names)
    y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    plt.figure(figsize=(7, 6))
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC={roc_auc[i]:.2f})', linewidth=1.1)
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Multi-class ROC Curve')
    plt.legend(loc="lower right", frameon=False)
    plt.grid(False)
    plt.tight_layout()
    if save_path:
        if save_path.endswith('.png'):
            save_path = save_path.replace('.png', '.svg')
        plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()



def compute_class_weights(labels):
    """Calculate aggressive class weights to handle class imbalance"""
    class_counts = np.bincount(labels)
    total_samples = len(labels)
    # Use square root scaling
    weights = total_samples / (len(class_counts) * np.sqrt(class_counts))
    return torch.FloatTensor(weights)


def color_sensitive_augmentation(image):
    """Apply color-sensitive augmentation for jaundice detection"""
    # Convert to HSV space for enhancement
    hsv_img = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)

    # Split channels
    h, s, v = cv2.split(hsv_img)

    # Create mask for yellow regions
    yellow_mask = ((h >= 20) & (h <= 60) & (s >= 0.4 * 255) & (v >= 0.4 * 255))

    # Apply random adjustments to non-yellow regions
    non_yellow_mask = ~yellow_mask
    h_shift = np.random.uniform(-5, 5)  # Slight hue adjustment
    s_scale = np.random.uniform(0.8, 1.2)  # Saturation adjustment
    v_scale = np.random.uniform(0.8, 1.2)  # Brightness adjustment

    h[non_yellow_mask] = np.clip(h[non_yellow_mask] + h_shift, 0, 179)
    s[non_yellow_mask] = np.clip(s[non_yellow_mask] * s_scale, 0, 255)
    v[non_yellow_mask] = np.clip(v[non_yellow_mask] * v_scale, 0, 255)

    # Merge channels and convert back to RGB
    hsv_img = cv2.merge([h, s, v])
    rgb_img = cv2.cvtColor(hsv_img.astype(np.uint8), cv2.COLOR_HSV2RGB)

    return rgb_img


# ========== 2. Face Feature Extraction ==========

class Jaundice3ClassDataset_ImageOnly(Dataset):
    """åªä½¿ç”¨å›¾åƒçš„æ•°æ®é›†ç±»ï¼ˆç”¨äºå›¾åƒæ¨¡å‹è®­ç»ƒï¼‰"""

    def __init__(self, root_dir, patient_names, split='train',
                 bilirubin_csv=None, transform=None, target_size=(224, 224),
                 enable_undersample=False, undersample_count=None):
        self.root_dir = root_dir
        self.split = split
        self.transform = transform
        self.target_size = target_size
        self.label_names = ['mild', 'moderate', 'severe']
        self.class_to_idx = {name: i for i, name in enumerate(self.label_names)}
        self.samples = []

        # è¯»å–èƒ†çº¢ç´ æ•°æ®
        assert bilirubin_csv is not None, "Must provide bilirubin csv file path"
        df = pd.read_excel(bilirubin_csv)
        df.columns = df.columns.str.strip()
        self.serial2bil = dict(zip(df['åºå·'], df['26ã€æ€»èƒ†çº¢ç´ å€¼ï¼ˆumol/Lï¼‰']))

        for folder_name in patient_names:
            serial = extract_serial_number(folder_name)
            if serial is None or serial not in self.serial2bil:
                print(f"[Warning] {folder_name} not matched to table serial number, skipping")
                continue

            bil = float(self.serial2bil[serial])
            # ç›´æ¥ä¸‰åˆ†ç±»
            if bil < 171:
                label_name = 'mild'
            elif bil < 342:
                label_name = 'moderate'
            else:
                label_name = 'severe'
            label = self.class_to_idx[label_name]

            patient_folder = os.path.join(self.root_dir, folder_name)
            if not os.path.isdir(patient_folder):
                continue

            imgs = [f for f in os.listdir(patient_folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
            if not imgs:
                print(f"[Warning] No images in {patient_folder}, skipping")
                continue

            for img_name in imgs:
                img_path = os.path.join(patient_folder, img_name)
                self.samples.append((img_path, label, serial))

        # ä¸‹é‡‡æ ·å¤„ç†
        if self.split == 'train' and enable_undersample:
            print("Applying undersampling to balance classes...")
            label_to_samples = defaultdict(list)
            for sample in self.samples:
                label_to_samples[sample[1]].append(sample)
            if undersample_count is None:
                undersample_count = min(len(s) for s in label_to_samples.values())
            print(f"Undersample count per class: {undersample_count}")
            new_samples = []
            for label, group in label_to_samples.items():
                if len(group) >= undersample_count:
                    new_samples.extend(random.sample(group, undersample_count))
                else:
                    new_samples.extend(group)
            self.samples = new_samples
            print(f"After undersampling: {Counter([s[1] for s in self.samples])}")

        print(f"Loaded {len(self.samples)} samples. Class distribution:")
        print(Counter([s[1] for s in self.samples]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        try:
            img_path, label, serial = self.samples[idx]
            img = Image.open(img_path).convert('RGB')

            # Apply data augmentation for training
            if self.split == 'train':
                np_img = np.array(img)
                np_img = color_sensitive_augmentation(np_img)
                img = Image.fromarray(np_img)

            # Apply transform if provided
            if self.transform:
                img_tensor = self.transform(img)
            else:
                # Default transform
                transform = T.Compose([
                    T.Resize(self.target_size),
                    T.ToTensor(),
                    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
                ])
                img_tensor = transform(img)

            return img_tensor, torch.tensor(label), serial

        except Exception as e:
            print(f"Cannot process sample {idx}, error: {e}")
            return self.__getitem__(max(0, idx - 1))


class YellowFeatureDataset(Dataset):
    """åªåŒ…å«é»„ç–¸ç‰¹å¾çš„æ•°æ®é›†"""

    def __init__(self, root_dir, patient_names, bilirubin_csv=None,
                 enable_undersample=False, undersample_count=None):
        self.root_dir = root_dir
        self.ffe = FacialFeatureExtractor(use_mediapipe=True)

        # è¯»å–èƒ†çº¢ç´ æ ‡ç­¾
        df = pd.read_excel(bilirubin_csv)
        self.serial2bil = dict(zip(df['åºå·'], df['26ã€æ€»èƒ†çº¢ç´ å€¼ï¼ˆumol/Lï¼‰']))

        self.samples = []
        for folder_name in patient_names:
            serial = extract_serial_number(folder_name)
            if serial is None or serial not in self.serial2bil:
                continue

            bil = float(self.serial2bil[serial])
            # ä¸‰åˆ†ç±»æ ‡ç­¾
            if bil < 171:
                label = 0  # mild
            elif bil < 342:
                label = 1  # moderate
            else:
                label = 2  # severe

            # æ”¶é›†è¯¥æ‚£è€…çš„æ‰€æœ‰å›¾åƒ
            patient_folder = os.path.join(self.root_dir, folder_name)
            if not os.path.exists(patient_folder):
                continue

            imgs = [f for f in os.listdir(patient_folder)
                    if f.lower().endswith(('.jpg', '.png', '.jpeg'))]

            for img_name in imgs:
                img_path = os.path.join(patient_folder, img_name)
                self.samples.append((img_path, label, serial))

        print(f"Loaded {len(self.samples)} yellow feature samples. Class distribution:")
        labels = [sample[1] for sample in self.samples]
        print(Counter(labels))

        # ä¸‹é‡‡æ ·å¤„ç†
        if enable_undersample and undersample_count:
            print("Applying undersampling to yellow features...")
            self.samples = self._apply_undersampling(self.samples, undersample_count)

    def _apply_undersampling(self, samples, undersample_count):
        """å¯¹é»„ç–¸ç‰¹å¾æ•°æ®è¿›è¡Œä¸‹é‡‡æ ·"""
        print(f"Undersample count per class: {undersample_count}")

        # æŒ‰ç±»åˆ«åˆ†ç»„
        class_samples = {0: [], 1: [], 2: []}
        for sample in samples:
            class_samples[sample[1]].append(sample)

        # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œä¸‹é‡‡æ ·
        undersampled = []
        for class_id in [0, 1, 2]:
            class_data = class_samples[class_id]
            if len(class_data) > undersample_count:
                # éšæœºé€‰æ‹©
                selected = random.sample(class_data, undersample_count)
                undersampled.extend(selected)
            else:
                # å¦‚æœæ ·æœ¬ä¸è¶³ï¼Œå…¨éƒ¨ä¿ç•™
                undersampled.extend(class_data)

        # æ‰“å°ä¸‹é‡‡æ ·åçš„åˆ†å¸ƒ
        labels = [sample[1] for sample in undersampled]
        print(f"After undersampling: {Counter(labels)}")

        return undersampled

    def extract_yellow_features(self, image):
        """ä»å›¾åƒä¸­æå–8ç»´é»„ç–¸ç‰¹å¾"""
        try:
            # æå–å·¦å³çœ¼åŒºåŸŸ
            left_eye, right_eye = self.ffe.extract_sclera_patches(image)

            # ä»æ¯ä¸ªçœ¼éƒ¨åŒºåŸŸæå–4ä¸ªç‰¹å¾
            left_features = extract_yellow_metrics(left_eye)  # 4ç»´
            right_features = extract_yellow_metrics(right_eye)  # 4ç»´

            # åˆå¹¶ä¸º8ç»´ç‰¹å¾å‘é‡
            features = np.concatenate([left_features, right_features])

            # æ£€æŸ¥ç‰¹å¾æ˜¯å¦æœ‰æ•ˆ
            if np.any(np.isnan(features)) or np.any(np.isinf(features)):
                print("Warning: Invalid features detected, using zeros")
                return np.zeros(8)

            return features

        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(8)  # è¿”å›é›¶å‘é‡ä½œä¸ºé»˜è®¤å€¼

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label, serial = self.samples[idx]

        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(img_path).convert('RGB')

            # æå–é»„ç–¸ç‰¹å¾
            yellow_features = self.extract_yellow_features(image)

            return {
                'yellow_features': torch.FloatTensor(yellow_features),
                'label': label,  # ç›´æ¥è¿”å›intï¼Œä¸è½¬æ¢ä¸ºtensor
                'serial': serial,
                'img_path': img_path
            }
        except Exception as e:
            print(f"Error loading sample {idx}: {e}")
            # è¿”å›é»˜è®¤å€¼
            return {
                'yellow_features': torch.zeros(8),
                'label': 0,
                'serial': -1,
                'img_path': img_path
            }


class YellowFeatureClassifier(nn.Module):
    """ä¸“é—¨ç”¨äºé»„ç–¸ç‰¹å¾åˆ†ç±»çš„ç½‘ç»œ"""

    def __init__(self, input_dim=8, hidden_dim=64, num_classes=3, dropout=0.3):
        super(YellowFeatureClassifier, self).__init__()

        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)

class Jaundice3ClassDataset_Full(Dataset):
    """åŒ…å«å›¾åƒå’Œé»„ç–¸ç‰¹å¾çš„å®Œæ•´æ•°æ®é›†ç±»ï¼ˆç”¨äºDynamic Weight Fusionè®­ç»ƒï¼‰"""

    def __init__(self, root_dir, patient_names, split='train',
                 bilirubin_csv=None, transform=None,
                 enable_undersample=False, undersample_count=None):
        self.root_dir = root_dir
        self.split = split
        self.transform = transform
        self.label_names = ['mild', 'moderate', 'severe']
        self.class_to_idx = {name: i for i, name in enumerate(self.label_names)}

        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        self.ffe = FacialFeatureExtractor(use_mediapipe=True)

        self.samples = []

        # è¯»å–èƒ†çº¢ç´ æ•°æ®
        assert bilirubin_csv is not None, "Must provide bilirubin csv file path"
        df = pd.read_excel(bilirubin_csv)
        df.columns = df.columns.str.strip()
        self.serial2bil = dict(zip(df['åºå·'], df['26ã€æ€»èƒ†çº¢ç´ å€¼ï¼ˆumol/Lï¼‰']))

        for folder_name in patient_names:
            serial = extract_serial_number(folder_name)
            if serial is None or serial not in self.serial2bil:
                print(f"[Warning] {folder_name} not matched to table serial number, skipping")
                continue

            bil = float(self.serial2bil[serial])
            # ç›´æ¥ä¸‰åˆ†ç±»
            if bil < 171:
                label_name = 'mild'
            elif bil < 342:
                label_name = 'moderate'
            else:
                label_name = 'severe'
            label = self.class_to_idx[label_name]

            patient_folder = os.path.join(self.root_dir, folder_name)
            if not os.path.isdir(patient_folder):
                continue

            imgs = [f for f in os.listdir(patient_folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
            if not imgs:
                print(f"[Warning] No images in {patient_folder}, skipping")
                continue

            for img_name in imgs:
                img_path = os.path.join(patient_folder, img_name)
                self.samples.append((img_path, label, serial))

        # ä¸‹é‡‡æ ·å¤„ç†
        if self.split == 'train' and enable_undersample:
            print("Applying undersampling to full dataset...")
            label_to_samples = defaultdict(list)
            for sample in self.samples:
                label_to_samples[sample[1]].append(sample)
            if undersample_count is None:
                undersample_count = min(len(s) for s in label_to_samples.values())
            print(f"Undersample count per class: {undersample_count}")
            new_samples = []
            for label, group in label_to_samples.items():
                if len(group) >= undersample_count:
                    new_samples.extend(random.sample(group, undersample_count))
                else:
                    new_samples.extend(group)
            self.samples = new_samples
            print(f"After undersampling: {Counter([s[1] for s in self.samples])}")

        print(f"Loaded {len(self.samples)} full samples. Class distribution:")
        print(Counter([s[1] for s in self.samples]))

    def extract_yellow_features(self, image):
        """ä»PILå›¾åƒä¸­æå–é»„ç–¸ç‰¹å¾"""
        try:
            # æå–çœ¼éƒ¨åŒºåŸŸ
            left_eye, right_eye = self.ffe.extract_sclera_patches(image)

            # ç¡®ä¿åŒºåŸŸæå–æˆåŠŸ
            if left_eye is None or right_eye is None:
                # å›é€€åˆ°ç›´æ¥è£å‰ª
                w, h = image.size
                left_eye = image.crop((0, 0, w // 4, h // 4)) if left_eye is None else left_eye
                right_eye = image.crop((3 * w // 4, 0, w, h // 4)) if right_eye is None else right_eye

            # æå–é»„ç–¸æŒ‡æ ‡
            left_eye_yellow = extract_yellow_metrics(left_eye)
            right_eye_yellow = extract_yellow_metrics(right_eye)

            # åˆå¹¶ç‰¹å¾ (æ¯ä¸ªçœ¼éƒ¨4ä¸ªç‰¹å¾ï¼Œæ€»å…±8ä¸ª)
            features = np.concatenate([left_eye_yellow, right_eye_yellow])
            return features

        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            return np.zeros(8)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        try:
            img_path, label, serial = self.samples[idx]
            img = Image.open(img_path).convert('RGB')

            # Apply data augmentation for training
            if self.split == 'train':
                np_img = np.array(img)
                np_img = color_sensitive_augmentation(np_img)
                img = Image.fromarray(np_img)

            # æå–é»„ç–¸ç‰¹å¾ï¼ˆåœ¨å˜æ¢ä¹‹å‰ï¼‰
            yellow_features = self.extract_yellow_features(img)

            # Apply transform if provided
            if self.transform:
                img_tensor = self.transform(img)
            else:
                # Default transform
                transform = T.Compose([
                    T.Resize((224, 224)),
                    T.ToTensor(),
                    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
                ])
                img_tensor = transform(img)

            return {
                'image': img_tensor,
                'yellow_features': torch.FloatTensor(yellow_features),
                'label': torch.tensor(label),
                'serial': serial
            }

        except Exception as e:
            print(f"Cannot process sample {idx}, error: {e}")
            return self.__getitem__(max(0, idx - 1))

def train_yellow_classifier(train_loader, val_loader, device, config):
    """è®­ç»ƒé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨"""

    # åˆ›å»ºæ¨¡å‹
    model = YellowFeatureClassifier(
        input_dim=8,  # 8ç»´é»„ç–¸ç‰¹å¾
        hidden_dim=config['yellow_classifier']['hidden_dim'],
        num_classes=3,
        dropout=config['yellow_classifier']['dropout']
    ).to(device)

    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†æ•°æ®ä¸å¹³è¡¡ï¼‰
    print("è®¡ç®—ç±»åˆ«æƒé‡...")
    all_labels = []
    for batch in train_loader:
        # ä¿®å¤ï¼šæ­£ç¡®æå–æ ‡ç­¾
        labels = batch['label']
        if isinstance(labels, torch.Tensor):
            all_labels.extend(labels.cpu().numpy().tolist())
        elif isinstance(labels, list):
            all_labels.extend(labels)
        else:
            all_labels.append(int(labels))

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_labels = np.array(all_labels, dtype=int)
    print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(all_labels)}")

    # è®¡ç®—ç±»åˆ«æƒé‡
    class_weights = compute_class_weights(all_labels).to(device)
    print(f"ç±»åˆ«æƒé‡: {class_weights}")

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config['yellow_classifier']['lr'],
        weight_decay=config['yellow_classifier']['weight_decay']
    )
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=10
    )

    best_f1 = 0
    patience_counter = 0

    print(f"å¼€å§‹è®­ç»ƒé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ï¼Œå…± {config['yellow_classifier']['epochs']} è½®...")

    for epoch in range(config['yellow_classifier']['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_idx, batch in enumerate(train_loader):
            features = batch['yellow_features'].to(device)  # [batch_size, 8]
            labels = batch['label']

            # ç¡®ä¿labelsæ˜¯tensoræ ¼å¼
            if not isinstance(labels, torch.Tensor):
                labels = torch.tensor(labels, dtype=torch.long)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(features)  # [batch_size, 3]
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            train_correct += (predicted == labels).sum().item()
            train_total += labels.size(0)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_preds = []
        val_labels = []
        val_loss = 0

        with torch.no_grad():
            for batch in val_loader:
                features = batch['yellow_features'].to(device)
                labels = batch['label']

                # ç¡®ä¿labelsæ˜¯tensoræ ¼å¼
                if not isinstance(labels, torch.Tensor):
                    labels = torch.tensor(labels, dtype=torch.long)
                labels = labels.to(device)

                outputs = model(features)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)

                val_preds.extend(predicted.cpu().numpy())
                val_labels.extend(labels.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        train_acc = train_correct / train_total
        val_acc = accuracy_score(val_labels, val_preds)
        val_f1 = f1_score(val_labels, val_preds, average='macro')

        scheduler.step(val_f1)

        # æ¯10è½®æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
        if epoch % 10 == 0 or epoch == config['yellow_classifier']['epochs'] - 1:
            print(f"Epoch {epoch + 1}/{config['yellow_classifier']['epochs']}: "
                  f"Train Loss: {train_loss / len(train_loader):.4f}, "
                  f"Train Acc: {train_acc:.4f}, "
                  f"Val Loss: {val_loss / len(val_loader):.4f}, "
                  f"Val Acc: {val_acc:.4f}, "
                  f"Val F1: {val_f1:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_f1 > best_f1:
            best_f1 = val_f1
            patience_counter = 0
            model_path = os.path.join(config['output_dir'], 'best_yellow_classifier.pth')
            torch.save(model.state_dict(), model_path)
            print(f"âœ“ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (F1: {best_f1:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= 20:  # æ—©åœ
                print(f"æ—©åœè§¦å‘ï¼Œæœ€ä½³F1: {best_f1:.4f}")
                break

    # åŠ è½½æœ€ä½³æ¨¡å‹
    print("åŠ è½½æœ€ä½³é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨...")
    model_path = os.path.join(config['output_dir'], 'best_yellow_classifier.pth')
    model.load_state_dict(torch.load(model_path, map_location=device))

    print(f"âœ“ é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨è®­ç»ƒå®Œæˆï¼Œæœ€ä½³F1: {best_f1:.4f}")
    return model


def train_ensemble_weights(image_models, yellow_model, train_loader, val_loader, device, config):
    """è®­ç»ƒé›†æˆæƒé‡ - ä½¿ç”¨LayeredEnsembleçš„ç®€å•ç‰ˆæœ¬"""

    ensemble_model = LayeredEnsemble(
        num_models=5,
        num_classes=3,
        fusion_method='weighted'
    ).to(device)

    optimizer = torch.optim.AdamW(ensemble_model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    best_f1 = 0
    for epoch in range(20):  # ç®€å•è®­ç»ƒ
        ensemble_model.train()
        for batch in train_loader:
            images = batch['image'].to(device)
            yellow_features = batch['yellow_features'].to(device)
            labels = batch['label'].to(device)

            # è·å–æ‰€æœ‰æ¨¡å‹logits
            logits_list = []
            for model in image_models.values():
                _, logits = model(images)
                logits_list.append(logits)

            yellow_logits = yellow_model(yellow_features)
            logits_list.append(yellow_logits)

            # é›†æˆé¢„æµ‹
            ensemble_output = ensemble_model(logits_list)
            loss = criterion(ensemble_output, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f"Ensemble Epoch {epoch + 1}/20 completed")

    return ensemble_model


class LayeredEnsemble(nn.Module):
    """åˆ†å±‚é›†æˆæ¨¡å‹"""

    def __init__(self, num_models=5, num_classes=3, fusion_method='weighted'):
        super(LayeredEnsemble, self).__init__()
        self.num_models = num_models  # 4ä¸ªå›¾åƒæ¨¡å‹ + 1ä¸ªé»„ç–¸ç‰¹å¾æ¨¡å‹
        self.num_classes = num_classes
        self.fusion_method = fusion_method

        if fusion_method == 'weighted':
            # å­¦ä¹ æƒé‡å‚æ•°
            self.weights = nn.Parameter(torch.ones(num_models) / num_models)

    def forward(self, logits_list):
        """
        logits_list: list of tensors, each tensor is [batch_size, num_classes]
        """
        # å †å æ‰€æœ‰logits
        stacked_logits = torch.stack(logits_list, dim=1)  # [batch_size, num_models, num_classes]

        if self.fusion_method == 'weighted':
            # åŠ æƒå¹³å‡
            weights = torch.softmax(self.weights, dim=0)
            weighted_logits = torch.sum(stacked_logits * weights.view(1, -1, 1), dim=1)
            return weighted_logits

        elif self.fusion_method == 'voting':
            # è½¯æŠ•ç¥¨
            averaged_logits = torch.mean(stacked_logits, dim=1)
            return averaged_logits

class DynamicWeightFusion(nn.Module):
    def __init__(self, num_models=6, num_classes=3, hidden_dim=64):  # æ›´æ–°ä¸º6ä¸ªæ¨¡å‹
        super().__init__()
        self.num_models = num_models  # 5ä¸ªå›¾åƒæ¨¡å‹ + 1ä¸ªé»„ç–¸æ¨¡å‹
        self.num_classes = num_classes

        # æƒé‡ç”Ÿæˆç½‘ç»œ
        self.weight_generator = nn.Sequential(
            nn.Linear(num_models * num_classes, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(hidden_dim // 2, num_models),
            nn.Softmax(dim=-1)
        )

        self.temperature = nn.Parameter(torch.tensor(1.0))

    def forward(self, logits):
        """
        Args:
            logits: [batch_size, num_models, num_classes] - 6ä¸ªæ¨¡å‹çš„logitsè¾“å‡º
        """
        batch_size = logits.size(0)
        flattened = logits.view(batch_size, -1)
        weights = self.weight_generator(flattened)
        weights = F.softmax(weights / self.temperature, dim=-1)
        weighted_logits = torch.sum(logits * weights.unsqueeze(-1), dim=1)
        return weighted_logits, weights

class FacialFeatureExtractor:
    def __init__(self, use_mediapipe=True):
        self.use_mediapipe = use_mediapipe
        if use_mediapipe:
            import mediapipe as mp
            self.mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = self.mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)

        # Define facial landmark indices
        self.left_eye_indices = list(range(362, 374)) + list(range(263, 273))
        self.right_eye_indices = list(range(133, 145)) + list(range(33, 43))

    def get_face_landmarks(self, image):
        if not self.use_mediapipe:
            return None, (image.height, image.width, 3) if isinstance(image, Image.Image) else image.shape

        if isinstance(image, Image.Image):
            image = np.array(image)
        try:
            results = self.face_mesh.process(image[..., ::-1])
            if results.multi_face_landmarks:
                return results.multi_face_landmarks[0].landmark, image.shape
        except Exception as e:
            print(f"Warning: Face landmark detection failed: {e}")
        return None, image.shape

    def extract_sclera_patches(self, image, expand_ratio=1.5):
        """Extract left and right eye patches from the image"""
        landmarks, img_shape = self.get_face_landmarks(image)

        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)

        if landmarks is None:
            # Fallback to basic cropping if landmarks not detected
            w, h = image.size
            size = min(w, h) // 6
            left_eye = image.crop((w // 4 - size, h // 3 - size, w // 4 + size, h // 3 + size))
            right_eye = image.crop((3 * w // 4 - size, h // 3 - size, 3 * w // 4 + size, h // 3 + size))
            return left_eye, right_eye

        height, width = img_shape[:2]

        def crop_eye(indices):
            pts = [(landmarks[idx].x * width, landmarks[idx].y * height) for idx in indices]
            min_x, max_x = min(p[0] for p in pts), max(p[0] for p in pts)
            min_y, max_y = min(p[1] for p in pts), max(p[1] for p in pts)
            cx, cy = (min_x + max_x) / 2, (min_y + max_y) / 2
            w, h = (max_x - min_x) * expand_ratio, (max_y - min_y) * expand_ratio
            return image.crop((max(0, int(cx - w / 2)), max(0, int(cy - h / 2)),
                               min(width, int(cx + w / 2)), min(height, int(cy + h / 2))))

        return crop_eye(self.left_eye_indices), crop_eye(self.right_eye_indices)


# ========== 3. Dataset Class ==========
class Jaundice3ClassDataset(Dataset):
    def __init__(self, root_dir, patient_names, split='train',
                 bilirubin_csv=None, transform=None, use_mediapipe=True,
                 include_yellow_features=True, enable_undersample=False, undersample_count=None):
        self.root_dir = root_dir
        self.split = split
        self.transform = transform
        self.include_yellow_features = include_yellow_features
        self.label_names = ['mild', 'moderate', 'severe']
        self.class_to_idx = {name: i for i, name in enumerate(self.label_names)}
        self.ffe = FacialFeatureExtractor(use_mediapipe=use_mediapipe) if include_yellow_features else None
        self.samples = []

        # è¯»å–èƒ†çº¢ç´ æ•°æ®
        assert bilirubin_csv is not None, "Must provide bilirubin csv file path"
        df = pd.read_excel(bilirubin_csv)
        df.columns = df.columns.str.strip()
        self.serial2bil = dict(zip(df['åºå·'], df['26ã€æ€»èƒ†çº¢ç´ å€¼ï¼ˆumol/Lï¼‰']))

        for folder_name in patient_names:
            serial = extract_serial_number(folder_name)
            if serial is None or serial not in self.serial2bil:
                print(f"[Warning] {folder_name} not matched to table serial number, skipping")
                continue

            bil = float(self.serial2bil[serial])
            # ç›´æ¥ä¸‰åˆ†ç±»
            if bil < 171:
                label_name = 'mild'
            elif bil < 342:
                label_name = 'moderate'
            else:
                label_name = 'severe'
            label = self.class_to_idx[label_name]

            patient_folder = os.path.join(self.root_dir, folder_name)
            if not os.path.isdir(patient_folder):
                continue

            imgs = [f for f in os.listdir(patient_folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
            if not imgs:
                print(f"[Warning] No images in {patient_folder}, skipping")
                continue

            for img_name in imgs:
                img_path = os.path.join(patient_folder, img_name)
                self.samples.append((img_path, label, serial))
        if self.split == 'train' and enable_undersample:
            print("Applying undersampling to balance classes...")
            label_to_samples = defaultdict(list)
            for sample in self.samples:
                label_to_samples[sample[1]].append(sample)
            if undersample_count is None:
                undersample_count = min(len(s) for s in label_to_samples.values())
            print(f"Undersample count per class: {undersample_count}")
            new_samples = []
            for label, group in label_to_samples.items():
                if len(group) >= undersample_count:
                    new_samples.extend(random.sample(group, undersample_count))
                else:
                    new_samples.extend(group)
            self.samples = new_samples
            print(f"After undersampling: {Counter([s[1] for s in self.samples])}")

        print(f"Loaded {len(self.samples)} samples. Class distribution:")
        print(Counter([s[1] for s in self.samples]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        try:
            img_path, label, serial = self.samples[idx]
            img = Image.open(img_path).convert('RGB')

            # Apply data augmentation for training
            if self.split == 'train':
                np_img = np.array(img)
                np_img = color_sensitive_augmentation(np_img)
                img = Image.fromarray(np_img)

            # Apply transform if provided
            if self.transform:
                img_tensor = self.transform(img)
            else:
                # Default transform
                transform = T.Compose([
                    T.Resize((224, 224)),
                    T.ToTensor(),
                    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
                ])
                img_tensor = transform(img)

            # Extract additional yellow features if needed
            if self.include_yellow_features and self.ffe:
                try:
                    # Extract eye regions
                    left_eye, right_eye = self.ffe.extract_sclera_patches(img)

                    # Ensure regions were extracted successfully
                    if left_eye is None or right_eye is None:
                        # Fallback to direct crops
                        w, h = img.size
                        left_eye = img.crop((0, 0, w // 4, h // 4)) if left_eye is None else left_eye
                        right_eye = img.crop((3 * w // 4, 0, w, h // 4)) if right_eye is None else right_eye

                    # Extract yellow metrics
                    left_eye_yellow = torch.tensor(extract_yellow_metrics(left_eye), dtype=torch.float32)
                    right_eye_yellow = torch.tensor(extract_yellow_metrics(right_eye), dtype=torch.float32)

                    # Combine yellow features
                    eyes_yellow = torch.cat([left_eye_yellow, right_eye_yellow])

                    return img_tensor, eyes_yellow, torch.tensor(label), serial

                except Exception as e:
                    print(f"Feature extraction error, file: {img_path}, error: {e}")
                    # Fall back to previous sample
                    return self.__getitem__(max(0, idx - 1))

            # If no additional features needed, return only the image tensor
            return img_tensor, torch.tensor(label), serial

        except Exception as e:
            print(f"Cannot process sample {idx}, error: {e}")
            # Try returning another sample index
            return self.__getitem__(max(0, idx - 1))  # Return previous valid sample


# ========== 4. SOTA Model Backbones ==========
class SOTABackbone(nn.Module):
    """Base class for SOTA model backbones"""

    def __init__(self, model_name, num_classes=3, pretrained=True, freeze_base=True):
        super().__init__()
        self.model_name = model_name
        self.num_classes = num_classes
        self.backbone = None
        self.feature_dim = 0
        self._build_model(pretrained, freeze_base)

    def _build_model(self, pretrained, freeze_base):
        """Build the model architecture - to be implemented by subclasses"""
        raise NotImplementedError

    def forward(self, x):
        """Forward pass - returns both features and logits"""
        raise NotImplementedError


class YOLOBackbone(SOTABackbone):
    def __init__(self, model_name, num_classes=3, pretrained=True, freeze_base=True, custom_model_path=None,
                 img_size=224):
        self.custom_model_path = custom_model_path
        self.img_size = img_size
        super().__init__(model_name, num_classes, pretrained, freeze_base)

    def _build_model(self, pretrained, freeze_base):
        from ultralytics import YOLO

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°é¢„è®­ç»ƒæƒé‡
        if self.custom_model_path and os.path.exists(self.custom_model_path):
            print(f"ğŸ¯ ä½¿ç”¨æœ¬åœ°YOLOé¢„è®­ç»ƒæƒé‡: {self.custom_model_path}")
            self.yolo_model = YOLO(self.custom_model_path)
            print(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°YOLOæ¨¡å‹: {self.custom_model_path}")
        else:
            if self.custom_model_path:
                print(f"âš ï¸ æœ¬åœ°YOLOæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.custom_model_path}")
            print("ğŸ“¥ ä½¿ç”¨é»˜è®¤YOLOæƒé‡...")
            self.yolo_model = YOLO('yolo11n-cls.pt')
            print("âœ… æˆåŠŸåŠ è½½é»˜è®¤YOLOv11åˆ†ç±»æ¨¡å‹")

        self.feature_dim = 1000
        self.classifier = nn.Linear(self.feature_dim, self.num_classes)

        if freeze_base:
            for param in self.yolo_model.model.parameters():
                param.requires_grad = False

    def train(self, mode=True):
        """é‡å†™trainæ–¹æ³•ï¼Œé¿å…è°ƒç”¨YOLOçš„trainæ–¹æ³•"""
        # åªè®¾ç½®PyTorchæ¨¡å—çš„è®­ç»ƒæ¨¡å¼
        self.classifier.train(mode)
        return self

    def eval(self):
        """é‡å†™evalæ–¹æ³•"""
        self.classifier.eval()
        return self

    def state_dict(self):
        """åªè¿”å›åˆ†ç±»å™¨çš„çŠ¶æ€å­—å…¸"""
        return {'classifier': self.classifier.state_dict()}

    def load_state_dict(self, state_dict, strict=True):
        """åªåŠ è½½åˆ†ç±»å™¨çš„çŠ¶æ€å­—å…¸"""
        if 'classifier' in state_dict:
            self.classifier.load_state_dict(state_dict['classifier'], strict=strict)
        else:
            # å…¼å®¹æ—§æ ¼å¼
            classifier_state = {k.replace('classifier.', ''): v for k, v in state_dict.items()
                                if k.startswith('classifier.')}
            if classifier_state:
                self.classifier.load_state_dict(classifier_state, strict=strict)

    def forward(self, x):
        batch_size = x.size(0)
        device = x.device  # ğŸ”¥ è·å–è¾“å…¥tensorçš„è®¾å¤‡

        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)
        x_denorm = x * std + mean
        x_denorm = torch.clamp(x_denorm, 0, 1)

        features_list = []
        for i in range(batch_size):
            img_tensor = x_denorm[i]
            img_np = img_tensor.permute(1, 2, 0).cpu().numpy()
            img_np = (img_np * 255).astype(np.uint8)
            img_np = cv2.resize(img_np, (self.img_size, self.img_size))

            try:
                with torch.no_grad():
                    # ğŸ”¥ å”¯ä¸€å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨è¾“å…¥tensorçš„è®¾å¤‡è€Œä¸æ˜¯å¼ºåˆ¶CPU
                    results = self.yolo_model.predict(img_np, verbose=False, device=device)

                if hasattr(results[0], 'probs') and results[0].probs is not None:
                    probs_data = results[0].probs.data

                    # å®‰å…¨çš„tensorè½¬æ¢
                    if isinstance(probs_data, torch.Tensor):
                        features = probs_data.clone().detach().float()
                    elif hasattr(probs_data, '__array__'):
                        features = torch.from_numpy(np.array(probs_data)).float()
                    else:
                        features = torch.tensor(probs_data, dtype=torch.float32)

                    # ğŸ”¥ ç¡®ä¿ç‰¹å¾åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                    features = features.to(device)

                    # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡®
                    current_dim = features.numel() if features.dim() > 0 else 0
                    if current_dim < self.feature_dim:
                        padding = torch.zeros(self.feature_dim - current_dim, device=device)  # ğŸ”¥ paddingä¹Ÿåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                        features = torch.cat([features.flatten(), padding])
                    elif current_dim > self.feature_dim:
                        features = features.flatten()[:self.feature_dim]
                    else:
                        features = features.flatten()
                else:
                    features = torch.zeros(self.feature_dim, dtype=torch.float32, device=device)  # ğŸ”¥ é»˜è®¤ç‰¹å¾åœ¨æ­£ç¡®è®¾å¤‡ä¸Š

            except Exception as e:
                print(f"YOLOé¢„æµ‹å¤±è´¥ (batch {i}): {e}")
                features = torch.zeros(self.feature_dim, dtype=torch.float32, device=device)  # ğŸ”¥ é”™è¯¯æƒ…å†µä¸‹ç‰¹å¾åœ¨æ­£ç¡®è®¾å¤‡ä¸Š

            features_list.append(features)

        # å †å æ‰€æœ‰ç‰¹å¾ï¼ˆç°åœ¨å·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼Œä¸éœ€è¦é¢å¤–çš„.to()ï¼‰
        features = torch.stack(features_list)
        logits = self.classifier(features)
        return features, logits


class ConvNextBackbone(SOTABackbone):
    """ConvNeXt model backbone"""

    def _build_model(self, pretrained, freeze_base):
        if pretrained:
            weights = 'DEFAULT'
        else:
            weights = None

        self.backbone = torchvision.models.convnext_base(weights=weights)
        self.feature_dim = 1024

        # Replace classifier with identity to get features
        self.original_classifier = self.backbone.classifier
        self.backbone.classifier = nn.Identity()

        # Create a new classifier
        self.classifier = nn.Linear(self.feature_dim, self.num_classes)

        # Freeze base if needed
        if freeze_base:
            for param in self.backbone.parameters():
                param.requires_grad = False

    def forward(self, x):
        features = self.backbone(x)  # æœŸæœ› [B, 1024]
        if features.ndim > 2:
            features = torch.flatten(features, 1)  # flatten æ‰€æœ‰ébatchç»´
        logits = self.classifier(features)
        return features, logits


class SwinTransformerBackbone(SOTABackbone):
    """Swin Transformer model backbone"""

    def _build_model(self, pretrained, freeze_base):
        if pretrained:
            weights = 'DEFAULT'
        else:
            weights = None

        self.backbone = torchvision.models.swin_v2_b(weights=weights)
        self.feature_dim = 1024

        # Store original head
        self.original_head = self.backbone.head

        # Remove classifier head to get features
        self.backbone.head = nn.Identity()

        # Create a new classifier
        self.classifier = nn.Linear(self.feature_dim, self.num_classes)

        # Freeze base if needed
        if freeze_base:
            for param in self.backbone.parameters():
                param.requires_grad = False

    def forward(self, x):
        features = self.backbone(x)
        if features.ndim > 2:
            features = torch.flatten(features, 1)
        logits = self.classifier(features)
        return features, logits


class EfficientNetBackbone(SOTABackbone):
    """EfficientNet model backbone"""

    def _build_model(self, pretrained, freeze_base):
        if pretrained:
            weights = 'DEFAULT'
        else:
            weights = None

        self.backbone = torchvision.models.efficientnet_v2_l(weights=weights)
        self.feature_dim = 1280

        # Store original classifier
        self.original_classifier = self.backbone.classifier

        # Remove classifier to get features
        self.backbone.classifier = nn.Identity()

        # Create a new classifier
        self.classifier = nn.Linear(self.feature_dim, self.num_classes)

        # Freeze base if needed
        if freeze_base:
            for param in self.backbone.parameters():
                param.requires_grad = False

    def forward(self, x):
        features = self.backbone(x)
        if features.ndim > 2:
            features = torch.flatten(features, 1)
        logits = self.classifier(features)
        return features, logits


class ViTBackbone(SOTABackbone):
    """Vision Transformer model backbone"""

    def _build_model(self, pretrained, freeze_base):
        if pretrained:
            weights = 'DEFAULT'
        else:
            weights = None

        self.backbone = torchvision.models.vit_b_16(weights=weights)
        self.feature_dim = 768

        # Store original head
        self.original_head = self.backbone.heads

        # Remove classifier head to get features
        self.backbone.heads = nn.Identity()

        # Create a new classifier
        self.classifier = nn.Linear(self.feature_dim, self.num_classes)

        # Freeze base if needed
        if freeze_base:
            for param in self.backbone.parameters():
                param.requires_grad = False

    def forward(self, x):
        features = self.backbone(x)
        if features.ndim > 2:
            features = torch.flatten(features, 1)
        logits = self.classifier(features)
        return features, logits


# ========== 5. Ensemble Model ==========
class MLPEnsemble(nn.Module):
    """MLP Ensemble model that combines features from multiple backbones"""

    def __init__(self, feature_dims, yellow_dim=8, num_classes=3, hidden_dims=[512, 256, 128]):
        super().__init__()

        # Calculate total input dimension
        total_dim = sum(feature_dims) + yellow_dim

        # Build MLP layers
        layers = []
        prev_dim = total_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.3))
            prev_dim = hidden_dim

        # Final classification layer
        layers.append(nn.Linear(prev_dim, num_classes))

        self.mlp = nn.Sequential(*layers)

    def forward(self, features_list, yellow=None):
        # Concatenate all features
        # Each element in features_list is a tensor of shape [batch_size, feature_dim]
        all_features = torch.cat(features_list, dim=1)

        # Add yellow features if provided
        if yellow is not None:
            all_features = torch.cat([all_features, yellow], dim=1)

        # Forward through MLP
        return self.mlp(all_features)

class MLPEnsembleLogits(nn.Module):
    def __init__(self, num_models, num_classes, yellow_dim=8, hidden_dims=[64, 32]):
        super().__init__()
        input_dim = num_models * num_classes + yellow_dim
        layers = []
        prev_dim = input_dim
        for h in hidden_dims:
            layers += [nn.Linear(prev_dim, h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(0.3)]
            prev_dim = h
        layers.append(nn.Linear(prev_dim, num_classes))
        self.mlp = nn.Sequential(*layers)

    def forward(self, logits_list, yellow=None):
        x = torch.cat(logits_list, dim=1)
        if yellow is not None:
            x = torch.cat([x, yellow], dim=1)
        return self.mlp(x)

class LogitsFusionEnsemble(nn.Module):
    def __init__(self, num_models, num_classes, hidden_dim=64, yellow_dim=8):
        super(LogitsFusionEnsemble, self).__init__()
        self.fusion_layer = DynamicWeightFusion(
            num_models=num_models,
            num_classes=num_classes,
            hidden_dim=hidden_dim
        )

    def forward(self, logits_list, yellow_features=None):
        # å †å logits
        stacked_logits = torch.stack(logits_list, dim=1)  # [B, num_models, num_classes]
        fused_logits, weights = self.fusion_layer(stacked_logits)
        return fused_logits, weights



class EarlyStopping:
    """æ—©åœæœºåˆ¶å®ç°"""

    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.verbose = verbose

        self.best_score = None
        self.counter = 0
        self.best_weights = None
        self.early_stop = False

    def __call__(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
                if self.restore_best_weights and self.best_weights is not None:
                    model.load_state_dict(self.best_weights)
                    if self.verbose:
                        print('Restored best weights')
        else:
            self.best_score = score
            self.save_checkpoint(model)
            self.counter = 0

    def save_checkpoint(self, model):
        """ä¿å­˜æ¨¡å‹æƒé‡"""
        if self.restore_best_weights:
            self.best_weights = model.state_dict().copy()


def train_yolo_classifier(train_loader, val_loader, device, config):
    """ä¸“é—¨è®­ç»ƒYOLOåˆ†ç±»å™¨ - ä¿®å¤ç‰ˆ"""

    print("å¼€å§‹è®­ç»ƒYOLOåˆ†ç±»å™¨...")
    print(f"ğŸ¯ ä½¿ç”¨æœ¬åœ°é¢„è®­ç»ƒæƒé‡: {config['yolo']['model_path']}")

    # åˆ›å»ºYOLOæ¨¡å‹ï¼Œç¡®ä¿ä½¿ç”¨æœ¬åœ°é¢„è®­ç»ƒæƒé‡
    yolo_model = YOLOBackbone(
        'YOLO',
        num_classes=3,
        pretrained=True,
        freeze_base=True,
        custom_model_path=config['yolo']['model_path'],
        img_size=config['yolo']['img_size']
    )
    yolo_model.to(device)

    # è®¡ç®—ç±»åˆ«æƒé‡
    all_labels = []
    for batch in train_loader:
        if isinstance(batch, dict):
            labels = batch['label']
        else:
            _, labels, _ = batch
        all_labels.extend(labels.cpu().numpy() if isinstance(labels, torch.Tensor) else labels)

    class_weights = compute_class_weights(np.array(all_labels)).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # åªä¼˜åŒ–åˆ†ç±»å™¨å±‚ï¼Œå› ä¸ºYOLO backboneæ˜¯å†»ç»“çš„
    optimizer = torch.optim.AdamW(
        yolo_model.classifier.parameters(),  # åªä¼˜åŒ–åˆ†ç±»å™¨
        lr=config.get('yolo', {}).get('lr', 0.001),
        weight_decay=config.get('yolo', {}).get('weight_decay', 1e-4)
    )

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=10
    )

    best_f1 = 0
    patience_counter = 0
    epochs = config.get('yolo', {}).get('epochs', 50)

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ - æ‰‹åŠ¨è®¾ç½®è®­ç»ƒæ¨¡å¼
        yolo_model.classifier.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch in train_loader:
            if isinstance(batch, dict):
                images = batch['image'].to(device)
                labels = batch['label'].to(device)
            else:
                images, labels, _ = batch
                images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            _, outputs = yolo_model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            train_correct += (predicted == labels).sum().item()
            train_total += labels.size(0)

        # éªŒè¯é˜¶æ®µ - æ‰‹åŠ¨è®¾ç½®è¯„ä¼°æ¨¡å¼
        yolo_model.classifier.eval()
        val_preds = []
        val_labels = []
        val_loss = 0

        with torch.no_grad():
            for batch in val_loader:
                if isinstance(batch, dict):
                    images = batch['image'].to(device)
                    labels = batch['label'].to(device)
                else:
                    images, labels, _ = batch
                    images, labels = images.to(device), labels.to(device)

                _, outputs = yolo_model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                val_preds.extend(predicted.cpu().numpy())
                val_labels.extend(labels.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        train_acc = train_correct / train_total
        val_acc = accuracy_score(val_labels, val_preds)
        val_f1 = f1_score(val_labels, val_preds, average='macro')

        scheduler.step(val_f1)

        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f"Epoch {epoch + 1}/{epochs}: "
                  f"Train Loss: {train_loss / len(train_loader):.4f}, "
                  f"Train Acc: {train_acc:.4f}, "
                  f"Val Loss: {val_loss / len(val_loader):.4f}, "
                  f"Val Acc: {val_acc:.4f}, "
                  f"Val F1: {val_f1:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹ - åªä¿å­˜åˆ†ç±»å™¨éƒ¨åˆ†
        if val_f1 > best_f1:
            best_f1 = val_f1
            patience_counter = 0
            model_path = config['existing_model_paths']['YOLO']
            # ä½¿ç”¨è‡ªå®šä¹‰çš„state_dictæ–¹æ³•
            torch.save(yolo_model.state_dict(), model_path)
            print(f"âœ“ YOLOåˆ†ç±»å™¨å·²ä¿å­˜ (F1: {best_f1:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= 20:
                print(f"æ—©åœè§¦å‘ï¼Œæœ€ä½³F1: {best_f1:.4f}")
                break

    # åŠ è½½æœ€ä½³æ¨¡å‹
    model_path = config['existing_model_paths']['YOLO']
    if os.path.exists(model_path):
        # ä½¿ç”¨è‡ªå®šä¹‰çš„load_state_dictæ–¹æ³•
        yolo_model.load_state_dict(torch.load(model_path, map_location=device))

    print(f"âœ… YOLOåˆ†ç±»å™¨è®­ç»ƒå®Œæˆï¼Œæœ€ä½³F1: {best_f1:.4f}")
    return yolo_model




def plot_training_history(history, config):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Lossæ›²çº¿
    axes[0, 0].plot(history['train_loss'], label='Train Loss', color='#4C72B0')
    axes[0, 0].plot(history['val_loss'], label='Val Loss', color='#C44E52')
    axes[0, 0].set_title('Loss Curves')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # Accuracyæ›²çº¿
    axes[0, 1].plot(history['train_acc'], label='Train Acc', color='#55A868')
    axes[0, 1].plot(history['val_acc'], label='Val Acc', color='#8172B3')
    axes[0, 1].set_title('Accuracy Curves')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # F1æ›²çº¿
    axes[1, 0].plot(history['train_f1'], label='Train F1', color='#CCB974')
    axes[1, 0].plot(history['val_f1'], label='Val F1', color='#64B5CD')
    axes[1, 0].set_title('F1 Score Curves')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('F1 Score')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # è¿‡æ‹Ÿåˆæ£€æµ‹
    train_val_gap = np.array(history['train_f1']) - np.array(history['val_f1'])
    axes[1, 1].plot(train_val_gap, label='Train-Val F1 Gap', color='#C44E52')
    axes[1, 1].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Overfitting Threshold')
    axes[1, 1].set_title('Overfitting Detection')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('F1 Gap')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    save_path = os.path.join(config['output_dir'], 'training_history.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"è®­ç»ƒå†å²å›¾å·²ä¿å­˜åˆ°: {save_path}")


# ========== 6. Training and Evaluation Functions ==========
def extract_all_logits(data_loader, image_models, yellow_model, device):
    """æå–æ‰€æœ‰æ¨¡å‹çš„logits"""
    all_logits = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="æå–logits"):
            images = batch['image'].to(device)
            yellow_features = batch['yellow_features'].to(device)
            labels = batch['label']

            batch_logits = []

            # å›¾åƒæ¨¡å‹logits
            for model in image_models.values():
                _, logits = model(images)
                batch_logits.append(logits.cpu())

            # é»„ç–¸æ¨¡å‹logits
            yellow_logits = yellow_model(yellow_features)
            batch_logits.append(yellow_logits.cpu())

            # å †å  [batch_size, num_models, num_classes]
            stacked_logits = torch.stack(batch_logits, dim=1)
            all_logits.append(stacked_logits)
            all_labels.append(labels)

    return torch.cat(all_logits, dim=0), torch.cat(all_labels, dim=0)


def train_dynamic_fusion_simple(train_logits, train_labels, val_logits, val_labels, device, config):
    """ç®€åŒ–çš„Dynamic Weight Fusionè®­ç»ƒ"""

    fusion_model = DynamicWeightFusion(
        num_models=6,  # 4ä¸ªå›¾åƒæ¨¡å‹ + 1ä¸ªé»„ç–¸æ¨¡å‹
        num_classes=3,
        hidden_dim=config['ensemble']['hidden_dim']
    ).to(device)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(train_logits, train_labels)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    val_dataset = TensorDataset(val_logits, val_labels)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # è®­ç»ƒé…ç½®
    optimizer = torch.optim.AdamW(fusion_model.parameters(), lr=config['ensemble']['lr'])
    criterion = nn.CrossEntropyLoss()

    best_f1 = 0
    patience_counter = 0

    for epoch in range(config['ensemble']['epochs']):
        # è®­ç»ƒ
        fusion_model.train()
        for logits_batch, labels_batch in train_loader:
            logits_batch = logits_batch.to(device)
            labels_batch = labels_batch.to(device)

            optimizer.zero_grad()
            fused_output, _ = fusion_model(logits_batch)
            loss = criterion(fused_output, labels_batch)
            loss.backward()
            optimizer.step()

        # éªŒè¯
        fusion_model.eval()
        val_preds = []
        val_labels_list = []

        with torch.no_grad():
            for logits_batch, labels_batch in val_loader:
                logits_batch = logits_batch.to(device)
                fused_output, _ = fusion_model(logits_batch)
                preds = fused_output.argmax(dim=1)
                val_preds.extend(preds.cpu().numpy())
                val_labels_list.extend(labels_batch.numpy())

        val_f1 = f1_score(val_labels_list, val_preds, average='macro')

        if val_f1 > best_f1:
            best_f1 = val_f1
            patience_counter = 0
            torch.save(fusion_model.state_dict(),
                       os.path.join(config['output_dir'], 'best_dynamic_fusion_yolo.pth'))
        else:
            patience_counter += 1
            if patience_counter >= config['ensemble']['patience']:
                break

        print(f"Epoch {epoch + 1}: Val F1 = {val_f1:.4f}")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    fusion_model.load_state_dict(torch.load(os.path.join(config['output_dir'], 'best_dynamic_fusion_yolo.pth')))
    return fusion_model


def plot_comprehensive_roc(unified_results, config):
    """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„ç»¼åˆROCæ›²çº¿"""

    plt.figure(figsize=(10, 8))
    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#64B5CD']

    for i, (model_name, result) in enumerate(unified_results.items()):
        y_true, y_pred, y_prob = result['predictions']

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROC
        n_classes = len(config['class_names'])
        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))

        # è®¡ç®—macro-average ROC AUC
        fpr = dict()
        tpr = dict()
        roc_auc = dict()

        for class_idx in range(n_classes):
            fpr[class_idx], tpr[class_idx], _ = roc_curve(y_true_bin[:, class_idx], y_prob[:, class_idx])
            roc_auc[class_idx] = auc(fpr[class_idx], tpr[class_idx])

        # è®¡ç®—macro-average ROC
        all_fpr = np.unique(np.concatenate([fpr[j] for j in range(n_classes)]))
        mean_tpr = np.zeros_like(all_fpr)
        for class_idx in range(n_classes):
            mean_tpr += np.interp(all_fpr, fpr[class_idx], tpr[class_idx])
        mean_tpr /= n_classes

        macro_auc = auc(all_fpr, mean_tpr)

        plt.plot(all_fpr, mean_tpr,
                 label=f'{model_name} (AUC = {macro_auc:.3f})',
                 color=colors[i % len(colors)], linewidth=2)

    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Comprehensive ROC Curves - All Models')
    plt.legend(loc="lower right", frameon=False)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    save_path = os.path.join(config['output_dir'], 'comprehensive_roc_curves.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"ç»¼åˆROCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")


def plot_comprehensive_prc(unified_results, config):
    """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„ç»¼åˆPRCæ›²çº¿å¯¹æ¯”"""
    from sklearn.metrics import precision_recall_curve, average_precision_score

    plt.figure(figsize=(10, 8))
    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#64B5CD']

    macro_aps = {}  # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„macro-average AP

    for i, (model_name, result) in enumerate(unified_results.items()):
        y_true, y_pred, y_prob = result['predictions']

        # è®¡ç®—macro-average PRC
        n_classes = len(config['class_names'])
        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„APç„¶åå¹³å‡
        class_aps = []
        for class_idx in range(n_classes):
            ap = average_precision_score(y_true_bin[:, class_idx], y_prob[:, class_idx])
            class_aps.append(ap)

        macro_ap = np.mean(class_aps)
        macro_aps[model_name] = macro_ap

        # è®¡ç®—macro-average precision-recallæ›²çº¿
        all_y_true = y_true_bin.ravel()
        all_y_scores = y_prob.ravel()
        macro_precision, macro_recall, _ = precision_recall_curve(all_y_true, all_y_scores)

        plt.plot(macro_recall, macro_precision,
                 label=f'{model_name} (mAP = {macro_ap:.3f})',
                 color=colors[i % len(colors)], linewidth=2)

    # æ·»åŠ åŸºçº¿ï¼ˆéšæœºåˆ†ç±»å™¨ï¼‰
    baseline_ap = np.mean([np.sum(y_true == i) / len(y_true) for i in range(n_classes)])
    plt.axhline(y=baseline_ap, color='gray', linestyle=':',
                label=f'Random Baseline (AP = {baseline_ap:.3f})')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Comprehensive Precision-Recall Curves - All Models')
    plt.legend(loc="lower left", frameon=False)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    save_path = os.path.join(config['output_dir'], 'comprehensive_prc_curves.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"ç»¼åˆPRCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
    return macro_aps


def plot_individual_model_roc(unified_results, config):
    """ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶å•ç‹¬çš„ROCæ›²çº¿"""

    for model_name, result in unified_results.items():
        y_true, y_pred, y_prob = result['predictions']

        plt.figure(figsize=(8, 6))
        n_classes = len(config['class_names'])
        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))

        colors = ['#4C72B0', '#55A868', '#C44E52']

        # ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„ROCæ›²çº¿
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])
            roc_auc = auc(fpr, tpr)

            plt.plot(fpr, tpr,
                     label=f'{config["class_names"][i]} (AUC = {roc_auc:.3f})',
                     color=colors[i], linewidth=2)

        # è®¡ç®—å¹¶ç»˜åˆ¶macro-average ROC
        fpr_macro = dict()
        tpr_macro = dict()
        for i in range(n_classes):
            fpr_macro[i], tpr_macro[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])

        all_fpr = np.unique(np.concatenate([fpr_macro[i] for i in range(n_classes)]))
        mean_tpr = np.zeros_like(all_fpr)
        for i in range(n_classes):
            mean_tpr += np.interp(all_fpr, fpr_macro[i], tpr_macro[i])
        mean_tpr /= n_classes

        macro_auc = auc(all_fpr, mean_tpr)
        plt.plot(all_fpr, mean_tpr,
                 label=f'Macro-average (AUC = {macro_auc:.3f})',
                 color='black', linestyle='--', linewidth=2)

        plt.plot([0, 1], [0, 1], 'k:', linewidth=1, label='Random')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'{model_name} - ROC Curves')
        plt.legend(loc="lower right", frameon=False)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        save_path = os.path.join(config['output_dir'], f'{model_name}_roc_curves.svg')
        plt.savefig(save_path, format='svg', bbox_inches='tight')
        plt.close()

        print(f"{model_name} ROCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")


def plot_individual_model_prc(unified_results, config):
    """ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶å•ç‹¬çš„PRCæ›²çº¿"""
    from sklearn.metrics import precision_recall_curve, average_precision_score

    for model_name, result in unified_results.items():
        y_true, y_pred, y_prob = result['predictions']

        plt.figure(figsize=(8, 6))
        n_classes = len(config['class_names'])
        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))

        colors = ['#4C72B0', '#55A868', '#C44E52']

        # ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„PRCæ›²çº¿
        class_aps = []
        for i in range(n_classes):
            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_prob[:, i])
            ap = average_precision_score(y_true_bin[:, i], y_prob[:, i])
            class_aps.append(ap)

            plt.plot(recall, precision,
                     label=f'{config["class_names"][i]} (AP = {ap:.3f})',
                     color=colors[i], linewidth=2)

        # è®¡ç®—å¹¶ç»˜åˆ¶macro-average PRC
        macro_ap = np.mean(class_aps)
        all_y_true = y_true_bin.ravel()
        all_y_scores = y_prob.ravel()
        macro_precision, macro_recall, _ = precision_recall_curve(all_y_true, all_y_scores)

        plt.plot(macro_recall, macro_precision,
                 label=f'Macro-average (AP = {macro_ap:.3f})',
                 color='black', linestyle='--', linewidth=2)

        # æ·»åŠ åŸºçº¿
        baseline_ap = np.mean([np.sum(y_true == i) / len(y_true) for i in range(n_classes)])
        plt.axhline(y=baseline_ap, color='gray', linestyle=':',
                    label=f'Baseline (AP = {baseline_ap:.3f})')

        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title(f'{model_name} - Precision-Recall Curves')
        plt.legend(loc="lower left", frameon=False)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        save_path = os.path.join(config['output_dir'], f'{model_name}_prc_curves.svg')
        plt.savefig(save_path, format='svg', bbox_inches='tight')
        plt.close()

        print(f"{model_name} PRCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")


def plot_confusion_matrices(unified_results, config):
    """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„æ··æ·†çŸ©é˜µå¯¹æ¯”"""

    n_models = len(unified_results)
    cols = 3
    rows = (n_models + cols - 1) // cols

    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))
    if n_models == 1:
        axes = [axes]
    elif rows == 1:
        axes = axes.reshape(1, -1)

    for idx, (model_name, result) in enumerate(unified_results.items()):
        row = idx // cols
        col = idx % cols
        ax = axes[row, col] if rows > 1 else axes[col]

        y_true, y_pred, y_prob = result['predictions']
        cm = confusion_matrix(y_true, y_pred)

        # è®¡ç®—ç™¾åˆ†æ¯”
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

        # ç»˜åˆ¶çƒ­å›¾
        im = ax.imshow(cm_percent, interpolation='nearest', cmap='Blues')

        # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
        thresh = cm_percent.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, f'{cm[i, j]}\n({cm_percent[i, j]:.1f}%)',
                        ha="center", va="center",
                        color="white" if cm_percent[i, j] > thresh else "black",
                        fontsize=10)

        ax.set_title(f'{model_name}')
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
        ax.set_xticks(range(len(config['class_names'])))
        ax.set_yticks(range(len(config['class_names'])))
        ax.set_xticklabels(config['class_names'])
        ax.set_yticklabels(config['class_names'])

    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(n_models, rows * cols):
        row = idx // cols
        col = idx % cols
        ax = axes[row, col] if rows > 1 else axes[col]
        ax.set_visible(False)

    plt.tight_layout()
    save_path = os.path.join(config['output_dir'], 'confusion_matrices_comparison.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"æ··æ·†çŸ©é˜µå¯¹æ¯”å·²ä¿å­˜åˆ°: {save_path}")


def plot_class_wise_performance(unified_results, config):
    """ç»˜åˆ¶å„ç±»åˆ«çš„æ€§èƒ½å¯¹æ¯”"""

    # å‡†å¤‡æ•°æ®
    models = list(unified_results.keys())
    classes = config['class_names']
    n_classes = len(classes)

    # æå–æ¯ä¸ªæ¨¡å‹æ¯ä¸ªç±»åˆ«çš„æ•æ„Ÿåº¦å’Œç‰¹å¼‚åº¦
    sensitivity_data = []
    specificity_data = []

    for model_name, result in unified_results.items():
        metrics = result['metrics']
        sensitivity_data.append(metrics['sensitivity_per_class'])
        specificity_data.append(metrics['specificity_per_class'])

    sensitivity_data = np.array(sensitivity_data)  # [n_models, n_classes]
    specificity_data = np.array(specificity_data)

    # ç»˜åˆ¶æ•æ„Ÿåº¦å¯¹æ¯”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    x = np.arange(n_classes)
    width = 0.15
    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#64B5CD']

    # æ•æ„Ÿåº¦
    for i, model_name in enumerate(models):
        ax1.bar(x + i * width, sensitivity_data[i], width,
                label=model_name, color=colors[i % len(colors)])

    ax1.set_xlabel('Classes')
    ax1.set_ylabel('Sensitivity')
    ax1.set_title('Class-wise Sensitivity Comparison')
    ax1.set_xticks(x + width * (len(models) - 1) / 2)
    ax1.set_xticklabels(classes)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1.05)

    # ç‰¹å¼‚åº¦
    for i, model_name in enumerate(models):
        ax2.bar(x + i * width, specificity_data[i], width,
                label=model_name, color=colors[i % len(colors)])

    ax2.set_xlabel('Classes')
    ax2.set_ylabel('Specificity')
    ax2.set_title('Class-wise Specificity Comparison')
    ax2.set_xticks(x + width * (len(models) - 1) / 2)
    ax2.set_xticklabels(classes)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 1.05)

    plt.tight_layout()
    save_path = os.path.join(config['output_dir'], 'class_wise_performance.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"ç±»åˆ«æ€§èƒ½å¯¹æ¯”å·²ä¿å­˜åˆ°: {save_path}")


def plot_model_performance_radar(unified_results, config):
    """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾"""

    # å‡†å¤‡æ•°æ®
    metrics = ['Accuracy', 'F1 Score', 'Sensitivity', 'Specificity', 'AUC']

    fig, axes = plt.subplots(2, 3, figsize=(18, 12), subplot_kw=dict(projection='polar'))
    axes = axes.flatten()

    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#64B5CD']

    for idx, (model_name, result) in enumerate(unified_results.items()):
        if idx >= len(axes):
            break

        ax = axes[idx]

        # æå–æŒ‡æ ‡å€¼
        model_metrics = result['metrics']
        values = [
            model_metrics['accuracy'],
            model_metrics['f1_macro'],
            model_metrics['sensitivity_mean'],
            model_metrics['specificity_mean'],
            model_metrics['auc']
        ]

        # è§’åº¦è®¾ç½®
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        values += values[:1]  # é—­åˆå›¾å½¢
        angles += angles[:1]

        # ç»˜åˆ¶é›·è¾¾å›¾
        ax.plot(angles, values, 'o-', linewidth=2,
                label=model_name, color=colors[idx % len(colors)])
        ax.fill(angles, values, alpha=0.25, color=colors[idx % len(colors)])

        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(0, 1)
        ax.set_title(f'{model_name} Performance', size=14, weight='bold')
        ax.grid(True)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for angle, value, metric in zip(angles[:-1], values[:-1], metrics):
            ax.text(angle, value + 0.05, f'{value:.3f}',
                    ha='center', va='center', fontsize=10)

    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(len(unified_results), len(axes)):
        axes[idx].set_visible(False)

    plt.tight_layout()
    save_path = os.path.join(config['output_dir'], 'model_performance_radar.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾å·²ä¿å­˜åˆ°: {save_path}")


def calc_ap_bootstrap(y_true, y_pred, y_prob):
    """è®¡ç®—APçš„Bootstrapå‡½æ•°"""
    from sklearn.metrics import average_precision_score
    from sklearn.preprocessing import label_binarize

    n_classes = len(np.unique(y_true))
    if n_classes > 2:
        y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
        class_aps = []
        for i in range(n_classes):
            ap = average_precision_score(y_true_bin[:, i], y_prob[:, i])
            class_aps.append(ap)
        return np.mean(class_aps)
    else:
        return average_precision_score(y_true, y_prob[:, 1])


def generate_bootstrap_comparison_table(bootstrap_results, config):
    """ç”ŸæˆBootstrapç»“æœå¯¹æ¯”è¡¨æ ¼"""

    # æ·»åŠ ç©ºå€¼æ£€æŸ¥
    if bootstrap_results is None:
        print("âŒ Bootstrapç»“æœä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆè¡¨æ ¼")
        return None

    print("\n" + "=" * 80)
    print("ğŸ”„ Bootstrapä¼°è®¡ç»“æœè¡¨ï¼ˆ500æ¬¡é‡é‡‡æ ·å‡å€¼ Â± 95% CIï¼‰")
    print("=" * 80)

    bootstrap_data = []
    metrics_list = ['Accuracy', 'F1', 'Sensitivity', 'Specificity', 'AUC', 'AP']

    for model_name, results in bootstrap_results.items():
        row = {'Model': model_name}
        for metric in metrics_list:
            if metric in results:
                mean, lower, upper = results[metric]
                row[metric] = f"{mean:.4f} ({lower:.4f}-{upper:.4f})"
            else:
                row[metric] = "N/A"
        bootstrap_data.append(row)

    bootstrap_df = pd.DataFrame(bootstrap_data)
    print(bootstrap_df.to_string(index=False))

    # ä¿å­˜è¡¨æ ¼
    bootstrap_path = os.path.join(config['output_dir'], 'bootstrap_metrics_comparison.csv')
    excel_path = os.path.join(config['output_dir'], 'bootstrap_metrics_comparison.xlsx')

    bootstrap_df.to_csv(bootstrap_path, index=False)
    bootstrap_df.to_excel(excel_path, index=False)

    print(f"\nğŸ“„ Bootstrapç»“æœè¡¨æ ¼å·²ä¿å­˜åˆ°:")
    print(f"  - {bootstrap_path}")
    print(f"  - {excel_path}")

    return bootstrap_df


def generate_methodology_report(config):
    """ç”Ÿæˆæ–¹æ³•å­¦è¯´æ˜æŠ¥å‘Š"""

    report_path = os.path.join(config['output_dir'], 'methodology_report.txt')

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Bootstrapæ–¹æ³•å­¦è¯´æ˜\n")
        f.write("=" * 50 + "\n\n")

        f.write("1. Bootstrapé‡é‡‡æ ·æ–¹æ³•\n")
        f.write("-" * 30 + "\n")
        f.write("- é‡é‡‡æ ·æ¬¡æ•°: 500æ¬¡\n")
        f.write("- ç½®ä¿¡åŒºé—´: 95%\n")
        f.write("- é‡‡æ ·æ–¹å¼: æœ‰æ”¾å›éšæœºé‡‡æ ·\n")
        f.write("- æ ·æœ¬å¤§å°: ä¸åŸå§‹æµ‹è¯•é›†ç›¸åŒ\n\n")

        f.write("2. æŒ‡æ ‡è®¡ç®—è¯´æ˜\n")
        f.write("-" * 30 + "\n")
        f.write("- BootstrapæŒ‡æ ‡: 500æ¬¡é‡é‡‡æ ·çš„å‡å€¼\n")
        f.write("- ç½®ä¿¡åŒºé—´: 2.5%å’Œ97.5%åˆ†ä½æ•°\n")
        f.write("- ç”¨é€”: è¯„ä¼°æŒ‡æ ‡çš„ç¨³å®šæ€§å’Œç»Ÿè®¡æ˜¾è‘—æ€§\n\n")

        f.write("3. ç»“æœè§£é‡Š\n")
        f.write("-" * 30 + "\n")
        f.write("- æŸ±çŠ¶å›¾æ˜¾ç¤º: Bootstrapå‡å€¼ Â± 95%ç½®ä¿¡åŒºé—´\n")
        f.write("- è¡¨æ ¼æ˜¾ç¤º: Bootstrapä¼°è®¡åŠç½®ä¿¡åŒºé—´\n")
        f.write("- ç½®ä¿¡åŒºé—´å®½åº¦: åæ˜ æŒ‡æ ‡çš„ä¸ç¡®å®šæ€§\n")
        f.write("- æ¨¡å‹æ¯”è¾ƒ: åŸºäºç½®ä¿¡åŒºé—´é‡å ç¨‹åº¦\n\n")

        f.write("4. ç»Ÿè®¡æ˜¾è‘—æ€§åˆ¤æ–­\n")
        f.write("-" * 30 + "\n")
        f.write("- ç½®ä¿¡åŒºé—´ä¸é‡å : æ¨¡å‹é—´å·®å¼‚æ˜¾è‘—\n")
        f.write("- ç½®ä¿¡åŒºé—´é‡å : æ¨¡å‹é—´å·®å¼‚å¯èƒ½ä¸æ˜¾è‘—\n")
        f.write("- ç½®ä¿¡åŒºé—´å®½åº¦: åæ˜ ä¼°è®¡çš„ä¸ç¡®å®šæ€§\n")
        f.write("- å»ºè®®: ç»“åˆå®é™…åº”ç”¨åœºæ™¯é€‰æ‹©æ¨¡å‹\n")

    print(f"ğŸ“– æ–¹æ³•å­¦è¯´æ˜å·²ä¿å­˜åˆ°: {report_path}")


def generate_final_summary_report_bootstrap(unified_results, bootstrap_results, config):
    """åŸºäºBootstrapç»“æœç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š"""

    # æ·»åŠ ç©ºå€¼æ£€æŸ¥
    if bootstrap_results is None:
        print("âŒ Bootstrapç»“æœä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹ç»“æœç”ŸæˆæŠ¥å‘Š")
        generate_final_summary_report(unified_results, config)
        return

    print("\n" + "=" * 80)
    print("ğŸ¯ DYNAMIC WEIGHT FUSION æœ€ç»ˆæ€»ç»“æŠ¥å‘Š (Bootstrapä¼°è®¡)")
    print("=" * 80)

    # åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨ï¼ˆåŸºäºBootstrapç»“æœï¼‰
    performance_table = PrettyTable()
    performance_table.field_names = [
        "Model", "Accuracy (95% CI)", "F1 (95% CI)", "Sensitivity (95% CI)", "Specificity (95% CI)", "AUC (95% CI)"
    ]

    best_metrics = {
        'accuracy': ('', 0),
        'f1': ('', 0),
        'sensitivity': ('', 0),
        'specificity': ('', 0),
        'auc': ('', 0)
    }

    for model_name, results in bootstrap_results.items():
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡ï¼ˆåŸºäºBootstrapå‡å€¼ï¼‰
        for metric_name, bootstrap_key in [
            ('accuracy', 'Accuracy'),
            ('f1', 'F1'),
            ('sensitivity', 'Sensitivity'),
            ('specificity', 'Specificity'),
            ('auc', 'AUC')
        ]:
            if bootstrap_key in results:
                current_value = results[bootstrap_key][0]  # Bootstrapå‡å€¼
                if current_value > best_metrics[metric_name][1]:
                    best_metrics[metric_name] = (model_name, current_value)

        # æ·»åŠ åˆ°è¡¨æ ¼ï¼ˆæ˜¾ç¤ºBootstrapç»“æœï¼‰
        acc_mean, acc_lower, acc_upper = results['Accuracy']
        f1_mean, f1_lower, f1_upper = results['F1']
        sens_mean, sens_lower, sens_upper = results['Sensitivity']
        spec_mean, spec_lower, spec_upper = results['Specificity']
        auc_mean, auc_lower, auc_upper = results['AUC']

        performance_table.add_row([
            model_name,
            f"{acc_mean:.4f} ({acc_lower:.4f}-{acc_upper:.4f})",
            f"{f1_mean:.4f} ({f1_lower:.4f}-{f1_upper:.4f})",
            f"{sens_mean:.4f} ({sens_lower:.4f}-{sens_upper:.4f})",
            f"{spec_mean:.4f} ({spec_lower:.4f}-{spec_upper:.4f})",
            f"{auc_mean:.4f} ({auc_lower:.4f}-{auc_upper:.4f})"
        ])

    print("\nğŸ“ˆ æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯” (Bootstrapä¼°è®¡):")
    print(performance_table)

    print("\nğŸ† æœ€ä½³æ€§èƒ½æŒ‡æ ‡ (Bootstrapå‡å€¼):")
    metric_display = {
        'accuracy': 'å‡†ç¡®ç‡',
        'f1': 'F1åˆ†æ•°',
        'sensitivity': 'æ•æ„Ÿåº¦',
        'specificity': 'ç‰¹å¼‚åº¦',
        'auc': 'AUC'
    }
    for metric_name, (model_name, value) in best_metrics.items():
        print(f"  {metric_display[metric_name]}: {model_name} ({value:.4f})")

    # Dynamic Fusionçš„ä¼˜åŠ¿åˆ†æ
    if 'DynamicFusion' in bootstrap_results:
        fusion_results = bootstrap_results['DynamicFusion']

        print(f"\nğŸš€ Dynamic Weight Fusion æ€§èƒ½åˆ†æ (Bootstrapä¼°è®¡):")
        print(f"  - å‡†ç¡®ç‡: {fusion_results['Accuracy'][0]:.4f} ({fusion_results['Accuracy'][1]:.4f}-{fusion_results['Accuracy'][2]:.4f})")
        print(f"  - F1åˆ†æ•°: {fusion_results['F1'][0]:.4f} ({fusion_results['F1'][1]:.4f}-{fusion_results['F1'][2]:.4f})")
        print(f"  - AUC: {fusion_results['AUC'][0]:.4f} ({fusion_results['AUC'][1]:.4f}-{fusion_results['AUC'][2]:.4f})")

        # ä¸æœ€ä½³å•æ¨¡å‹å¯¹æ¯”
        best_single_f1 = 0
        best_single_model = ""
        for model_name, results in bootstrap_results.items():
            if model_name != 'DynamicFusion':
                if results['F1'][0] > best_single_f1:
                    best_single_f1 = results['F1'][0]
                    best_single_model = model_name

        improvement = fusion_results['F1'][0] - best_single_f1
        print(f"\nğŸ’¡ ç›¸æ¯”æœ€ä½³å•æ¨¡å‹ ({best_single_model}):")
        print(f"  - F1åˆ†æ•°æå‡: {improvement:+.4f}")
        print(f"  - ç›¸å¯¹æå‡: {(improvement / best_single_f1) * 100:+.2f}%")

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = os.path.join(config['output_dir'], 'final_summary_report_bootstrap.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Dynamic Weight Fusion æœ€ç»ˆæ€»ç»“æŠ¥å‘Š (Bootstrapä¼°è®¡)\n")
        f.write("=" * 80 + "\n\n")

        f.write("æ¨¡å‹æ¶æ„:\n")
        f.write(f"- å›¾åƒæ¨¡å‹: {', '.join(config['models'])}\n")
        f.write(f"- é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨: 8ç»´ç‰¹å¾è¾“å…¥\n")
        f.write(f"- èåˆæ–¹æ³•: Dynamic Weight Fusion\n")
        f.write(f"- ç±»åˆ«æ•°: {len(config['class_names'])}\n")
        f.write(f"- è¯„ä¼°æ–¹æ³•: Bootstrapé‡é‡‡æ · (500æ¬¡è¿­ä»£)\n\n")

        f.write("æ€§èƒ½å¯¹æ¯” (Bootstrapä¼°è®¡):\n")
        f.write(str(performance_table) + "\n\n")

        f.write("æœ€ä½³æ€§èƒ½æŒ‡æ ‡ (Bootstrapå‡å€¼):\n")
        for metric_name, (model_name, value) in best_metrics.items():
            f.write(f"  {metric_display[metric_name]}: {model_name} ({value:.4f})\n")

        if 'DynamicFusion' in bootstrap_results:
            fusion_results = bootstrap_results['DynamicFusion']
            f.write(f"\nDynamic Weight Fusion è¯¦ç»†æ€§èƒ½ (Bootstrapä¼°è®¡):\n")
            f.write(f"  - å‡†ç¡®ç‡: {fusion_results['Accuracy'][0]:.4f} ({fusion_results['Accuracy'][1]:.4f}-{fusion_results['Accuracy'][2]:.4f})\n")
            f.write(f"  - F1åˆ†æ•°: {fusion_results['F1'][0]:.4f} ({fusion_results['F1'][1]:.4f}-{fusion_results['F1'][2]:.4f})\n")
            f.write(f"  - AUC: {fusion_results['AUC'][0]:.4f} ({fusion_results['AUC'][1]:.4f}-{fusion_results['AUC'][2]:.4f})\n")

            improvement = fusion_results['F1'][0] - best_single_f1
            f.write(f"\nç›¸æ¯”æœ€ä½³å•æ¨¡å‹æå‡:\n")
            f.write(f"  - F1åˆ†æ•°æå‡: {improvement:+.4f}\n")
            f.write(f"  - ç›¸å¯¹æå‡: {(improvement / best_single_f1) * 100:+.2f}%\n")

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    print("\nğŸ‰ Dynamic Weight Fusion è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:")
    print("  - unified_model_comparison_with_ap.svg: Bootstrapç½®ä¿¡åŒºé—´æŸ±çŠ¶å›¾ï¼ˆå«APï¼‰")
    print("  - bootstrap_metrics_comparison.csv/xlsx: Bootstrapç»“æœè¡¨æ ¼")
    print("  - comprehensive_roc_curves.svg: ç»¼åˆROCæ›²çº¿")
    print("  - comprehensive_prc_curves.svg: ç»¼åˆPRCæ›²çº¿")
    print("  - [ModelName]_roc_curves.svg: å„æ¨¡å‹å•ç‹¬ROCæ›²çº¿")
    print("  - [ModelName]_prc_curves.svg: å„æ¨¡å‹å•ç‹¬PRCæ›²çº¿")
    print("  - confusion_matrices_comparison.svg: æ··æ·†çŸ©é˜µå¯¹æ¯”")
    print("  - class_wise_performance.svg: ç±»åˆ«æ€§èƒ½å¯¹æ¯”")
    print("  - model_performance_radar.svg: æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾")
    print("  - methodology_report.txt: æ–¹æ³•å­¦è¯´æ˜")
    print("  - final_summary_report_bootstrap.txt: æœ€ç»ˆæ€»ç»“æŠ¥å‘Š")


def train_and_evaluate_dynamic_fusion_comprehensive(config):
    """å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼° - åŒ…å«æ‰€æœ‰å¯è§†åŒ–åˆ†æ"""

    # é¦–å…ˆè¿è¡ŒåŸºç¡€çš„Dynamic Weight Fusionè®­ç»ƒ
    print("ğŸ”¥ å¼€å§‹Dynamic Weight FusionåŸºç¡€è®­ç»ƒ...")
    fusion_model, test_results = train_and_evaluate_dynamic_fusion(config)

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_dir = os.path.join(config['dataset_dir'], "test")
    test_names = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]

    full_test_dataset = Jaundice3ClassDataset_Full(
        test_dir, test_names, split='test',
        bilirubin_csv=config['bilirubin_csv'],
        transform=T.Compose([
            T.Resize((224, 224)), T.ToTensor(),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    )
    full_test_loader = DataLoader(
        full_test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers']
    )

    # åŠ è½½æ‰€æœ‰æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ‰€æœ‰åŸºç¡€æ¨¡å‹...")

    # åŠ è½½å›¾åƒæ¨¡å‹
    image_models = {}

    for model_name in config['models']:
        print(f"åŠ è½½ {model_name} æ¨¡å‹...")

        if model_name == 'ConvNext':
            model = ConvNextBackbone(model_name, num_classes=3, pretrained=True, freeze_base=True)
        elif model_name == 'Swin':
            model = SwinTransformerBackbone(model_name, num_classes=3, pretrained=True, freeze_base=True)
        elif model_name == 'EfficientNet':
            model = EfficientNetBackbone(model_name, num_classes=3, pretrained=True, freeze_base=True)
        elif model_name == 'ViT':
            model = ViTBackbone(model_name, num_classes=3, pretrained=True, freeze_base=True)
        elif model_name == 'YOLO':  # ä¿®å¤ï¼šæ­£ç¡®çš„ç¼©è¿›
            yolo_model_path = config['existing_model_paths']['YOLO']

            if os.path.exists(yolo_model_path):
                # å¦‚æœå­˜åœ¨è®­ç»ƒå¥½çš„YOLOæƒé‡ï¼ŒåŠ è½½å®ƒ
                model = YOLOBackbone(
                    model_name='YOLO',
                    num_classes=3,
                    pretrained=True,
                    freeze_base=True,
                    custom_model_path=config['yolo']['model_path'],  # é¢„è®­ç»ƒæƒé‡è·¯å¾„
                    img_size=config['yolo']['img_size']
                )
                model.load_state_dict(torch.load(yolo_model_path, map_location=device))
                print(f"âœ… åŠ è½½å·²è®­ç»ƒçš„YOLOæƒé‡: {yolo_model_path}")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„YOLOæƒé‡: {yolo_model_path}")
                print("ä½¿ç”¨é¢„è®­ç»ƒYOLOæƒé‡ç»§ç»­ï¼Œå»ºè®®å…ˆè¿è¡ŒåŸºç¡€è®­ç»ƒç”ŸæˆYOLOæ¨¡å‹")

                # åˆ›å»ºYOLOæ¨¡å‹å®ä¾‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼‰
                model = YOLOBackbone(
                    model_name='YOLO',
                    num_classes=3,
                    pretrained=True,
                    freeze_base=True,
                    custom_model_path=config['yolo']['model_path'],
                    img_size=config['yolo']['img_size']
                )

            model.to(device)
            model.eval()
            image_models[model_name] = model
            continue
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_name}")

        # åŠ è½½å…¶ä»–æ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡
        model_path = config['existing_model_paths'].get(model_name)
        if model_path and os.path.exists(model_path):
            try:
                model.load_state_dict(torch.load(model_path, map_location=device))
                print(f"âœ… ä» {model_path} åŠ è½½äº† {model_name} æƒé‡")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ {model_name} æƒé‡å¤±è´¥: {e}")
                print(f"å°†ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ç»§ç»­...")

        model.to(device)
        model.eval()
        image_models[model_name] = model
        print(f"âœ… {model_name} æ¨¡å‹åŠ è½½å®Œæˆ")

    # åŠ è½½é»„ç–¸æ¨¡å‹
    print("åŠ è½½é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨...")
    yellow_model = YellowFeatureClassifier(input_dim=8, hidden_dim=64, num_classes=3, dropout=0.3)
    yellow_model_path = os.path.join(config['output_dir'], 'best_yellow_classifier.pth')

    if not os.path.exists(yellow_model_path):
        print("âŒ æœªæ‰¾åˆ°é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ï¼Œéœ€è¦å…ˆè®­ç»ƒ...")
        print("å»ºè®®å…ˆè¿è¡ŒåŸºç¡€è®­ç»ƒç”Ÿæˆé»„ç–¸åˆ†ç±»å™¨")
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°é»„ç–¸åˆ†ç±»å™¨: {yellow_model_path}")

    yellow_model.load_state_dict(torch.load(yellow_model_path, map_location=device))
    yellow_model.to(device)
    yellow_model.eval()
    print("âœ… é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨åŠ è½½å®Œæˆ")

    # ç»Ÿä¸€è¯„ä¼°æ‰€æœ‰æ¨¡å‹
    print("\nğŸ” ç»Ÿä¸€è¯„ä¼°æ‰€æœ‰æ¨¡å‹...")
    unified_results = unified_evaluate_all_models(
        image_models, yellow_model, fusion_model, full_test_loader, device, config
    )

    # ç”Ÿæˆæ‰€æœ‰å¯¹æ¯”å›¾è¡¨å’Œåˆ†æ
    print("\nğŸ“Š ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–åˆ†æ...")

    # 1. Bootstrapåˆ†æï¼ˆåŒ…å«APï¼‰- ç¡®ä¿è¿”å›ç»“æœ
    print("ğŸ”„ è®¡ç®—Bootstrapç½®ä¿¡åŒºé—´ï¼ˆ500æ¬¡é‡é‡‡æ ·ï¼ŒåŒ…å«APï¼‰...")
    bootstrap_results = create_unified_bootstrap_chart_with_ap(unified_results, config)

    # æ£€æŸ¥bootstrap_resultsæ˜¯å¦æˆåŠŸè¿”å›
    if bootstrap_results is None:
        print("âŒ Bootstrapåˆ†æå¤±è´¥ï¼Œè·³è¿‡ç›¸å…³æ­¥éª¤")
        return unified_results, None

    # 2. Bootstrapç»“æœè¡¨æ ¼
    print("ğŸ“‹ ç”ŸæˆBootstrapç»“æœè¡¨æ ¼...")
    bootstrap_df = generate_bootstrap_comparison_table(bootstrap_results, config)

    # 3. ç»¼åˆROCæ›²çº¿
    print("ğŸ“ˆ ç»˜åˆ¶ç»¼åˆROCæ›²çº¿...")
    plot_comprehensive_roc(unified_results, config)

    # 4. ç»¼åˆPRCæ›²çº¿
    print("ğŸ“ˆ ç»˜åˆ¶ç»¼åˆPRCæ›²çº¿...")
    macro_aps = plot_comprehensive_prc(unified_results, config)

    # 5. å•æ¨¡å‹ROCæ›²çº¿
    print("ğŸ“ˆ ç»˜åˆ¶å•æ¨¡å‹ROCæ›²çº¿...")
    plot_individual_model_roc(unified_results, config)

    # 6. å•æ¨¡å‹PRCæ›²çº¿
    print("ğŸ“ˆ ç»˜åˆ¶å•æ¨¡å‹PRCæ›²çº¿...")
    plot_individual_model_prc(unified_results, config)

    # 7. æ··æ·†çŸ©é˜µå¯¹æ¯”
    print("ğŸ“Š ç»˜åˆ¶æ··æ·†çŸ©é˜µå¯¹æ¯”...")
    plot_confusion_matrices(unified_results, config)

    # 8. ç±»åˆ«æ€§èƒ½å¯¹æ¯”
    print("ğŸ“Š ç»˜åˆ¶ç±»åˆ«æ€§èƒ½å¯¹æ¯”...")
    plot_class_wise_performance(unified_results, config)

    # 9. æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾
    print("ğŸ“Š ç»˜åˆ¶æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾...")
    plot_model_performance_radar(unified_results, config)

    # 10. ç”Ÿæˆè¯´æ˜æŠ¥å‘Š
    generate_methodology_report(config)

    # 11. ä¿å­˜è¯¦ç»†ç»“æœ
    save_comprehensive_results(unified_results, config)

    # 12. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_final_summary_report_bootstrap(unified_results, bootstrap_results, config)

    return unified_results, bootstrap_results


def generate_final_summary_report(unified_results, config):
    """ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š"""

    print("\n" + "=" * 80)
    print("ğŸ¯ DYNAMIC WEIGHT FUSION æœ€ç»ˆæ€»ç»“æŠ¥å‘Š")
    print("=" * 80)

    # åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨
    performance_table = PrettyTable()
    performance_table.field_names = [
        "Model", "Accuracy", "F1 (Macro)", "Sensitivity", "Specificity", "AUC"
    ]

    best_metrics = {
        'accuracy': ('', 0),
        'f1_macro': ('', 0),
        'sensitivity_mean': ('', 0),
        'specificity_mean': ('', 0),
        'auc': ('', 0)
    }

    for model_name, result in unified_results.items():
        metrics = result['metrics']

        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        for metric_name, current_value in [
            ('accuracy', metrics['accuracy']),
            ('f1_macro', metrics['f1_macro']),
            ('sensitivity_mean', metrics['sensitivity_mean']),
            ('specificity_mean', metrics['specificity_mean']),
            ('auc', metrics['auc'])
        ]:
            if current_value > best_metrics[metric_name][1]:
                best_metrics[metric_name] = (model_name, current_value)

        # æ·»åŠ åˆ°è¡¨æ ¼
        performance_table.add_row([
            model_name,
            f"{metrics['accuracy']:.4f}",
            f"{metrics['f1_macro']:.4f}",
            f"{metrics['sensitivity_mean']:.4f}",
            f"{metrics['specificity_mean']:.4f}",
            f"{metrics['auc']:.4f}"
        ])

    print("\nğŸ“ˆ æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print(performance_table)

    print("\nğŸ† æœ€ä½³æ€§èƒ½æŒ‡æ ‡:")
    for metric_name, (model_name, value) in best_metrics.items():
        metric_display = {
            'accuracy': 'å‡†ç¡®ç‡',
            'f1_macro': 'F1åˆ†æ•°',
            'sensitivity_mean': 'æ•æ„Ÿåº¦',
            'specificity_mean': 'ç‰¹å¼‚åº¦',
            'auc': 'AUC'
        }
        print(f"  {metric_display[metric_name]}: {model_name} ({value:.4f})")

    # Dynamic Fusionçš„ä¼˜åŠ¿åˆ†æ
    if 'DynamicFusion' in unified_results:
        fusion_metrics = unified_results['DynamicFusion']['metrics']

        print(f"\nğŸš€ Dynamic Weight Fusion æ€§èƒ½åˆ†æ:")
        print(f"  - å‡†ç¡®ç‡: {fusion_metrics['accuracy']:.4f}")
        print(f"  - F1åˆ†æ•°: {fusion_metrics['f1_macro']:.4f}")
        print(f"  - AUC: {fusion_metrics['auc']:.4f}")

        # ä¸æœ€ä½³å•æ¨¡å‹å¯¹æ¯”
        best_single_f1 = 0
        best_single_model = ""
        for model_name, result in unified_results.items():
            if model_name != 'DynamicFusion':
                if result['metrics']['f1_macro'] > best_single_f1:
                    best_single_f1 = result['metrics']['f1_macro']
                    best_single_model = model_name

        improvement = fusion_metrics['f1_macro'] - best_single_f1
        print(f"\nğŸ’¡ ç›¸æ¯”æœ€ä½³å•æ¨¡å‹ ({best_single_model}):")
        print(f"  - F1åˆ†æ•°æå‡: {improvement:+.4f}")
        print(f"  - ç›¸å¯¹æå‡: {(improvement / best_single_f1) * 100:+.2f}%")

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = os.path.join(config['output_dir'], 'final_summary_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Dynamic Weight Fusion æœ€ç»ˆæ€»ç»“æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")

        f.write("æ¨¡å‹æ¶æ„:\n")
        f.write(f"- å›¾åƒæ¨¡å‹: {', '.join(config['models'])}\n")
        f.write(f"- é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨: 8ç»´ç‰¹å¾è¾“å…¥\n")
        f.write(f"- èåˆæ–¹æ³•: Dynamic Weight Fusion\n")
        f.write(f"- ç±»åˆ«æ•°: {len(config['class_names'])}\n\n")

        f.write("æ€§èƒ½å¯¹æ¯”:\n")
        f.write(str(performance_table) + "\n\n")

        f.write("æœ€ä½³æ€§èƒ½æŒ‡æ ‡:\n")
        for metric_name, (model_name, value) in best_metrics.items():
            metric_display = {
                'accuracy': 'å‡†ç¡®ç‡',
                'f1_macro': 'F1åˆ†æ•°',
                'sensitivity_mean': 'æ•æ„Ÿåº¦',
                'specificity_mean': 'ç‰¹å¼‚åº¦',
                'auc': 'AUC'
            }
            f.write(f"  {metric_display[metric_name]}: {model_name} ({value:.4f})\n")

        if 'DynamicFusion' in unified_results:
            fusion_metrics = unified_results['DynamicFusion']['metrics']
            f.write(f"\nDynamic Weight Fusion è¯¦ç»†æ€§èƒ½:\n")
            f.write(f"  - å‡†ç¡®ç‡: {fusion_metrics['accuracy']:.4f}\n")
            f.write(f"  - F1åˆ†æ•°: {fusion_metrics['f1_macro']:.4f}\n")
            f.write(f"  - AUC: {fusion_metrics['auc']:.4f}\n")

            improvement = fusion_metrics['f1_macro'] - best_single_f1
            f.write(f"\nç›¸æ¯”æœ€ä½³å•æ¨¡å‹æå‡:\n")
            f.write(f"  - F1åˆ†æ•°æå‡: {improvement:+.4f}\n")
            f.write(f"  - ç›¸å¯¹æå‡: {(improvement / best_single_f1) * 100:+.2f}%\n")

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    print("\nğŸ‰ Dynamic Weight Fusion è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:")
    print("  - unified_model_comparison.svg: Bootstrapç½®ä¿¡åŒºé—´æŸ±çŠ¶å›¾")
    print("  - comprehensive_roc_curves.svg: ç»¼åˆROCæ›²çº¿")
    print("  - model_comparison_detailed.csv/xlsx: è¯¦ç»†å¯¹æ¯”è¡¨æ ¼")
    print("  - comprehensive_results.json: å®Œæ•´ç»“æœæ•°æ®")
    print("  - final_summary_report.txt: æœ€ç»ˆæ€»ç»“æŠ¥å‘Š")


def generate_comparison_table(unified_results, config):
    """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”è¡¨æ ¼"""

    # åˆ›å»ºDataFrame
    data = []
    for model_name, result in unified_results.items():
        metrics = result['metrics']
        data.append({
            'Model': model_name,
            'Accuracy': f"{metrics['accuracy']:.4f}",
            'F1 (Macro)': f"{metrics['f1_macro']:.4f}",
            'F1 (Weighted)': f"{metrics['f1_weighted']:.4f}",
            'Sensitivity': f"{metrics['sensitivity_mean']:.4f}",
            'Specificity': f"{metrics['specificity_mean']:.4f}",
            'AUC': f"{metrics['auc']:.4f}"
        })

    df = pd.DataFrame(data)

    # ä¿å­˜CSVå’ŒExcel
    csv_path = os.path.join(config['output_dir'], 'model_comparison_detailed.csv')
    excel_path = os.path.join(config['output_dir'], 'model_comparison_detailed.xlsx')

    df.to_csv(csv_path, index=False)
    df.to_excel(excel_path, index=False)

    print(f"è¯¦ç»†å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜åˆ°:")
    print(f"  - {csv_path}")
    print(f"  - {excel_path}")

    # æ‰“å°è¡¨æ ¼
    print("\n" + "=" * 80)
    print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨")
    print("=" * 80)
    print(df.to_string(index=False))


def train_and_evaluate_dynamic_fusion(config):
    """ä¸»è®­ç»ƒå‡½æ•°ï¼šä¸“æ³¨äºlogitsèåˆçš„é›†æˆå­¦ä¹ """

    set_seed(config['seed'])
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    os.makedirs(config['output_dir'], exist_ok=True)

    # æ•°æ®å‡†å¤‡
    train_dir = os.path.join(config['dataset_dir'], "train")
    val_dir = os.path.join(config['dataset_dir'], "val")
    test_dir = os.path.join(config['dataset_dir'], "test")

    train_names = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]
    val_names = [d for d in os.listdir(val_dir) if os.path.isdir(os.path.join(val_dir, d))]
    test_names = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]

    print(f"æ•°æ®é›†å¤§å° - Train: {len(train_names)}, Val: {len(val_names)}, Test: {len(test_names)}")

    # =================== å…ˆåˆ›å»ºæ•°æ®åŠ è½½å™¨ ===================
    print("\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")

    transforms = T.Compose([
        T.Resize((224, 224)), T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    # è®­ç»ƒé›†
    train_dataset = Jaundice3ClassDataset_Full(
        train_dir, train_names, split='train',
        bilirubin_csv=config['bilirubin_csv'],
        transform=transforms,
        enable_undersample=True,
        undersample_count=200
    )

    # éªŒè¯é›†
    val_dataset = Jaundice3ClassDataset_Full(
        val_dir, val_names, split='val',
        bilirubin_csv=config['bilirubin_csv'],
        transform=transforms
    )

    # æµ‹è¯•é›†
    test_dataset = Jaundice3ClassDataset_Full(
        test_dir, test_names, split='test',
        bilirubin_csv=config['bilirubin_csv'],
        transform=transforms
    )

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],
                              shuffle=True, num_workers=config['num_workers'])
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'],
                            shuffle=False, num_workers=config['num_workers'])
    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'],
                             shuffle=False, num_workers=config['num_workers'])

    print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")

    # =================== åŠ è½½å·²è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ ===================
    print("\n" + "=" * 50)
    print("åŠ è½½å·²è®­ç»ƒçš„åŸºç¡€æ¨¡å‹")
    print("=" * 50)

    image_models = {}

    for model_name in config['models']:
        print(f"åŠ è½½ {model_name} æ¨¡å‹...")

        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        if model_name == 'ConvNext':
            model = ConvNextBackbone(model_name, num_classes=3, pretrained=True, freeze_base=True)
        elif model_name == 'Swin':
            model = SwinTransformerBackbone(model_name, num_classes=3, pretrained=True, freeze_base=True)
        elif model_name == 'EfficientNet':
            model = EfficientNetBackbone(model_name, num_classes=3, pretrained=True, freeze_base=True)
        elif model_name == 'ViT':
            model = ViTBackbone(model_name, num_classes=3, pretrained=True, freeze_base=True)
        elif model_name == 'YOLO':  # ä¿®å¤ï¼šæ­£ç¡®çš„ç¼©è¿›
            yolo_model_path = config['existing_model_paths']['YOLO']

            if os.path.exists(yolo_model_path):
                print(f"âœ“ å‘ç°å·²è®­ç»ƒçš„YOLOæ¨¡å‹: {yolo_model_path}")
                model = YOLOBackbone(
                    'YOLO',
                    num_classes=3,
                    pretrained=True,
                    freeze_base=True,
                    custom_model_path=config['yolo']['model_path'],
                    img_size=config['yolo']['img_size']
                )
                model.to(device)
                # ä½¿ç”¨è‡ªå®šä¹‰çš„load_state_dictæ–¹æ³•
                model.load_state_dict(torch.load(yolo_model_path, map_location=device))
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„YOLOæƒé‡: {yolo_model_path}")
                print("å°†ä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¿›è¡Œè‡ªåŠ¨è®­ç»ƒ...")

                # ä¸ºYOLOè®­ç»ƒåˆ›å»ºä¸“é—¨çš„æ•°æ®åŠ è½½å™¨ï¼ˆåªåŒ…å«å›¾åƒï¼‰
                yolo_train_dataset = Jaundice3ClassDataset_ImageOnly(
                    train_dir, train_names, split='train',
                    bilirubin_csv=config['bilirubin_csv'],
                    transform=transforms,
                    enable_undersample=True,
                    undersample_count=200
                )
                yolo_val_dataset = Jaundice3ClassDataset_ImageOnly(
                    val_dir, val_names, split='val',
                    bilirubin_csv=config['bilirubin_csv'],
                    transform=transforms
                )

                yolo_train_loader = DataLoader(yolo_train_dataset, batch_size=config['batch_size'],
                                               shuffle=True, num_workers=config['num_workers'])
                yolo_val_loader = DataLoader(yolo_val_dataset, batch_size=config['batch_size'],
                                             shuffle=False, num_workers=config['num_workers'])

                # è®­ç»ƒYOLOæ¨¡å‹
                model = train_yolo_classifier(yolo_train_loader, yolo_val_loader, device, config)
                torch.save(model.state_dict(), yolo_model_path)
                print(f"âœ… YOLOæ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜åˆ°: {yolo_model_path}")

            model.to(device)
            model.eval()
            image_models[model_name] = model
            continue
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_name}")

        # åŠ è½½å…¶ä»–æ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡
        model_path = config['existing_model_paths'][model_name]
        if os.path.exists(model_path):
            model.load_state_dict(torch.load(model_path, map_location=device))
            model.to(device)
            model.eval()
            image_models[model_name] = model
            print(f"âœ“ {model_name} æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")

    # åŠ è½½é»„ç–¸ç‰¹å¾æ¨¡å‹
    print("åŠ è½½é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨...")
    yellow_model = YellowFeatureClassifier(input_dim=8, hidden_dim=64, num_classes=3, dropout=0.3)
    yellow_model_path = os.path.join(config['output_dir'], 'best_yellow_classifier.pth')

    if os.path.exists(yellow_model_path):
        yellow_model.load_state_dict(torch.load(yellow_model_path, map_location=device))
        yellow_model.to(device)
        yellow_model.eval()
        print("âœ“ é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨åŠ è½½æˆåŠŸ")
    else:
        print("æœªæ‰¾åˆ°é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ï¼Œéœ€è¦å…ˆè®­ç»ƒ...")
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°é»„ç–¸åˆ†ç±»å™¨: {yellow_model_path}")

    # =================== è®­ç»ƒLogitsèåˆé›†æˆæ¨¡å‹ ===================
    print("\n" + "=" * 50)
    print("æå–æ‰€æœ‰æ¨¡å‹çš„logitså¹¶è®­ç»ƒDynamic Weight Fusion")
    print("=" * 50)

    # æå–è®­ç»ƒæ•°æ®logits
    print("æå–è®­ç»ƒæ•°æ®logits...")
    train_logits, train_labels = extract_all_logits(train_loader, image_models, yellow_model, device)

    print("æå–éªŒè¯æ•°æ®logits...")
    val_logits, val_labels = extract_all_logits(val_loader, image_models, yellow_model, device)

    # è®­ç»ƒDynamic Weight Fusion
    print("è®­ç»ƒDynamic Weight Fusionæ¨¡å‹...")
    fusion_model = train_dynamic_fusion_simple(
        train_logits, train_labels, val_logits, val_labels, device, config
    )

    # æµ‹è¯•è¯„ä¼°
    print("æå–æµ‹è¯•æ•°æ®logits...")
    test_logits, test_labels = extract_all_logits(test_loader, image_models, yellow_model, device)

    print("è¯„ä¼°Dynamic Weight Fusionæ¨¡å‹...")
    test_results = evaluate_dynamic_fusion(fusion_model, test_logits, test_labels, device, config)

    return fusion_model, test_results



def calc_ap_bootstrap(y_true, y_pred, y_prob):
    """è®¡ç®—APçš„Bootstrapå‡½æ•°"""
    from sklearn.metrics import average_precision_score
    from sklearn.preprocessing import label_binarize

    n_classes = len(np.unique(y_true))
    if n_classes > 2:
        y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
        class_aps = []
        for i in range(n_classes):
            ap = average_precision_score(y_true_bin[:, i], y_prob[:, i])
            class_aps.append(ap)
        return np.mean(class_aps)
    else:
        return average_precision_score(y_true, y_prob[:, 1])


def create_unified_bootstrap_chart_with_ap(unified_results, config):
    """åˆ›å»ºåŒ…å«APçš„ç»Ÿä¸€BootstrapæŸ±çŠ¶å›¾"""

    print("ğŸ”„ å¼€å§‹Bootstrapé‡é‡‡æ ·åˆ†æï¼ˆåŒ…å«APæŒ‡æ ‡ï¼‰...")

    metrics = ['Accuracy', 'F1', 'Sensitivity', 'Specificity', 'AUC', 'AP']
    models = list(unified_results.keys())
    n_metrics = len(metrics)
    n_models = len(models)

    # è®¡ç®—Bootstrapç½®ä¿¡åŒºé—´
    bootstrap_results = {}

    try:
        for model_name, result in unified_results.items():
            print(f"  å¤„ç†æ¨¡å‹: {model_name}")
            y_true, y_pred, y_prob = result['predictions']
            bootstrap_results[model_name] = {}

            for metric in metrics:
                try:
                    if metric == 'Accuracy':
                        mean, (lower, upper) = bootstrap_metric(
                            y_true, y_pred, None,
                            lambda y_true, y_pred, *args: accuracy_score(y_true, y_pred)
                        )
                    elif metric == 'F1':
                        mean, (lower, upper) = bootstrap_metric(
                            y_true, y_pred, None,
                            lambda y_true, y_pred, *args: f1_score(y_true, y_pred, average='macro')
                        )
                    elif metric == 'Sensitivity':
                        mean, (lower, upper) = bootstrap_metric(
                            y_true, y_pred, None, calc_sensitivity_bootstrap
                        )
                    elif metric == 'Specificity':
                        mean, (lower, upper) = bootstrap_metric(
                            y_true, y_pred, None, calc_specificity_bootstrap
                        )
                    elif metric == 'AUC':
                        mean, (lower, upper) = bootstrap_metric(
                            y_true, y_pred, y_prob, calc_auc_bootstrap
                        )
                    elif metric == 'AP':
                        mean, (lower, upper) = bootstrap_metric(
                            y_true, y_pred, y_prob, calc_ap_bootstrap
                        )

                    bootstrap_results[model_name][metric] = (mean, lower, upper)
                    print(f"    âœ“ {metric}: {mean:.4f} ({lower:.4f}-{upper:.4f})")

                except Exception as e:
                    print(f"    âŒ {metric} è®¡ç®—å¤±è´¥: {e}")
                    bootstrap_results[model_name][metric] = (0.0, 0.0, 0.0)

        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        plt.figure(figsize=(14, 8))
        bar_width = 0.12
        index = np.arange(n_models)
        colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#FF7F0E']

        for i, metric in enumerate(metrics):
            means = []
            lowers = []
            uppers = []

            for model_name in models:
                mean, lower, upper = bootstrap_results[model_name][metric]
                means.append(mean)
                lowers.append(lower)
                uppers.append(upper)

            means = np.array(means)
            lowers = np.array(lowers)
            uppers = np.array(uppers)
            yerr = np.vstack([means - lowers, uppers - means])

            plt.bar(
                index + i * bar_width,
                means,
                bar_width,
                label=metric,
                color=colors[i % len(colors)],
                yerr=yerr,
                capsize=3,
                edgecolor='black',
                linewidth=0.8,
                error_kw={'elinewidth': 1.0, 'capthick': 1.0}
            )

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, value in enumerate(means):
                plt.text(
                    j + i * bar_width,
                    uppers[j] + 0.01,
                    f'{value:.3f}',
                    ha='center',
                    va='bottom',
                    fontsize=8,
                    rotation=90
                )

        plt.xlabel('Models', fontsize=12)
        plt.ylabel('Bootstrap Mean Â± 95% CI', fontsize=12)
        plt.title('Comprehensive Model Comparison with AP (Bootstrap 95% CI)', fontsize=14)
        plt.xticks(index + bar_width * (n_metrics - 1) / 2, models, rotation=15)
        plt.ylim(0, 1.05)
        plt.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, -0.08), ncol=6)
        plt.tight_layout(rect=[0, 0.07, 1, 1])

        save_path = os.path.join(config['output_dir'], 'unified_model_comparison_with_ap.svg')
        plt.savefig(save_path, format='svg', bbox_inches='tight')
        plt.close()

        print(f"âœ… åŒ…å«APçš„ç»Ÿä¸€æŸ±çŠ¶å›¾å·²ä¿å­˜åˆ°: {save_path}")
        print(f"âœ… Bootstrapåˆ†æå®Œæˆï¼Œè¿”å› {len(bootstrap_results)} ä¸ªæ¨¡å‹çš„ç»“æœ")

        return bootstrap_results

    except Exception as e:
        print(f"âŒ Bootstrapåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None



def plot_multiclass_prc(y_true, y_score, class_names, save_path=None):
    """ç»˜åˆ¶å¤šç±»åˆ«PRCæ›²çº¿"""
    from sklearn.metrics import precision_recall_curve, average_precision_score
    import seaborn as sns

    n_classes = len(class_names)
    y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))

    plt.figure(figsize=(10, 8))
    colors = custom_palette[:n_classes]

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„PRC
    precision = dict()
    recall = dict()
    average_precision = dict()

    for i in range(n_classes):
        precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_score[:, i])
        average_precision[i] = average_precision_score(y_true_bin[:, i], y_score[:, i])

        plt.plot(recall[i], precision[i],
                 label=f'{class_names[i]} (AP={average_precision[i]:.3f})',
                 color=colors[i], linewidth=2)

    # è®¡ç®—macro-average PRC
    all_y_true = y_true_bin.ravel()
    all_y_scores = y_score.ravel()
    macro_precision, macro_recall, _ = precision_recall_curve(all_y_true, all_y_scores)
    macro_ap = average_precision_score(all_y_true, all_y_scores)

    plt.plot(macro_recall, macro_precision,
             label=f'Macro-average (AP={macro_ap:.3f})',
             color='black', linestyle='--', linewidth=2)

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Multi-class Precision-Recall Curve')
    plt.legend(loc="lower left", frameon=False)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    if save_path:
        if save_path.endswith('.png'):
            save_path = save_path.replace('.png', '.svg')
        plt.savefig(save_path, format='svg', bbox_inches='tight')
        print(f"PRCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")

    plt.close()

    return average_precision


def plot_comprehensive_prc(unified_results, config):
    """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„ç»¼åˆPRCæ›²çº¿å¯¹æ¯”"""
    from sklearn.metrics import precision_recall_curve, average_precision_score

    plt.figure(figsize=(12, 8))
    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#64B5CD']

    macro_aps = {}  # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„macro-average AP

    for i, (model_name, result) in enumerate(unified_results.items()):
        y_true, y_pred, y_prob = result['predictions']

        # è®¡ç®—macro-average PRC
        n_classes = len(config['class_names'])
        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))

        # æ–¹æ³•1ï¼šè®¡ç®—æ¯ä¸ªç±»åˆ«çš„APç„¶åå¹³å‡
        class_aps = []
        for class_idx in range(n_classes):
            ap = average_precision_score(y_true_bin[:, class_idx], y_prob[:, class_idx])
            class_aps.append(ap)

        macro_ap = np.mean(class_aps)
        macro_aps[model_name] = macro_ap

        # è®¡ç®—macro-average precision-recallæ›²çº¿
        all_y_true = y_true_bin.ravel()
        all_y_scores = y_prob.ravel()
        macro_precision, macro_recall, _ = precision_recall_curve(all_y_true, all_y_scores)

        plt.plot(macro_recall, macro_precision,
                 label=f'{model_name} (mAP = {macro_ap:.3f})',
                 color=colors[i % len(colors)], linewidth=2)

    # æ·»åŠ åŸºçº¿ï¼ˆéšæœºåˆ†ç±»å™¨ï¼‰
    baseline_ap = np.mean([np.sum(y_true == i) / len(y_true) for i in range(n_classes)])
    plt.axhline(y=baseline_ap, color='gray', linestyle=':',
                label=f'Random Baseline (AP = {baseline_ap:.3f})')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Comprehensive Precision-Recall Curves - All Models')
    plt.legend(loc="lower left", frameon=False)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    save_path = os.path.join(config['output_dir'], 'comprehensive_prc_curves.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"ç»¼åˆPRCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
    return macro_aps


def plot_class_specific_prc(unified_results, config):
    """ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶è¯¦ç»†çš„PRCæ›²çº¿"""
    from sklearn.metrics import precision_recall_curve, average_precision_score

    n_classes = len(config['class_names'])
    fig, axes = plt.subplots(1, n_classes, figsize=(5 * n_classes, 5))
    if n_classes == 1:
        axes = [axes]

    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#64B5CD']

    class_aps = {class_name: {} for class_name in config['class_names']}

    for class_idx, class_name in enumerate(config['class_names']):
        ax = axes[class_idx]

        for model_idx, (model_name, result) in enumerate(unified_results.items()):
            y_true, y_pred, y_prob = result['predictions']

            # äºŒå€¼åŒ–æ ‡ç­¾
            y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))

            # è®¡ç®—è¯¥ç±»åˆ«çš„PRC
            precision, recall, _ = precision_recall_curve(
                y_true_bin[:, class_idx], y_prob[:, class_idx]
            )
            ap = average_precision_score(y_true_bin[:, class_idx], y_prob[:, class_idx])

            class_aps[class_name][model_name] = ap

            ax.plot(recall, precision,
                    label=f'{model_name} (AP={ap:.3f})',
                    color=colors[model_idx % len(colors)], linewidth=2)

        # æ·»åŠ åŸºçº¿
        baseline_ap = np.sum(y_true == class_idx) / len(y_true)
        ax.axhline(y=baseline_ap, color='gray', linestyle=':',
                   label=f'Baseline (AP={baseline_ap:.3f})')

        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('Recall')
        ax.set_ylabel('Precision')
        ax.set_title(f'{class_name} - Precision-Recall Curve')
        ax.legend(loc="lower left", frameon=False, fontsize=9)
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    save_path = os.path.join(config['output_dir'], 'class_specific_prc_curves.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"ç±»åˆ«ä¸“ç”¨PRCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
    return class_aps


def calc_ap_bootstrap(y_true, y_pred, y_prob):
    """è®¡ç®—APçš„Bootstrapå‡½æ•° - å®‰å…¨ç‰ˆ"""
    try:
        from sklearn.metrics import average_precision_score
        from sklearn.preprocessing import label_binarize

        # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        y_prob = np.array(y_prob)

        # è·å–ç±»åˆ«æ•°
        unique_classes = np.unique(y_true)
        n_classes = len(unique_classes)

        if n_classes > 2:
            # å¤šåˆ†ç±»æƒ…å†µ
            y_true_bin = label_binarize(y_true, classes=unique_classes)

            # ç¡®ä¿y_probçš„ç»´åº¦æ­£ç¡®
            if y_prob.ndim == 1:
                raise ValueError("y_probåº”è¯¥æ˜¯2Dæ•°ç»„ï¼Œå¯¹äºå¤šåˆ†ç±»é—®é¢˜")

            class_aps = []
            for i in range(n_classes):
                try:
                    ap = average_precision_score(y_true_bin[:, i], y_prob[:, i])
                    class_aps.append(ap)
                except Exception as e:
                    print(f"    è­¦å‘Š: ç±»åˆ« {i} çš„APè®¡ç®—å¤±è´¥: {e}")
                    class_aps.append(0.0)

            return np.mean(class_aps)
        else:
            # äºŒåˆ†ç±»æƒ…å†µ
            if y_prob.ndim == 2:
                return average_precision_score(y_true, y_prob[:, 1])
            else:
                return average_precision_score(y_true, y_prob)

    except Exception as e:
        print(f"    APè®¡ç®—å¤±è´¥: {e}")
        return 0.0


# Quick test function to debug the issue
def test_bootstrap_inputs(unified_results):
    """æµ‹è¯•Bootstrapè¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§"""
    print("\nğŸ” æ£€æŸ¥Bootstrapè¾“å…¥æ•°æ®...")

    for model_name, result in unified_results.items():
        print(f"\næ¨¡å‹: {model_name}")

        try:
            y_true, y_pred, y_prob = result['predictions']

            print(f"  y_true shape: {np.array(y_true).shape}")
            print(f"  y_pred shape: {np.array(y_pred).shape}")
            print(f"  y_prob shape: {np.array(y_prob).shape}")
            print(f"  Unique classes in y_true: {np.unique(y_true)}")
            print(f"  Unique classes in y_pred: {np.unique(y_pred)}")

            # æ£€æŸ¥y_probçš„æœ‰æ•ˆæ€§
            y_prob_array = np.array(y_prob)
            if y_prob_array.ndim == 2:
                print(f"  y_prob sum per row (should be ~1.0): {y_prob_array.sum(axis=1)[:5]}...")

        except Exception as e:
            print(f"  âŒ æ•°æ®æ£€æŸ¥å¤±è´¥: {e}")


def evaluate_logits_fusion_ensemble(image_models, yellow_model, ensemble_model,
                                   test_loader, device, config):
    """è¯„ä¼°logitsèåˆé›†æˆæ¨¡å‹"""

    print("å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")

    # è®¾ç½®æ‰€æœ‰æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    for model in image_models.values():
        model.eval()
    yellow_model.eval()
    ensemble_model.eval()

    all_preds = []
    all_labels = []
    all_probs = []
    all_attention_weights = []

    # ç”¨äºåˆ†æå„ä¸ªæ¨¡å‹çš„è´¡çŒ®
    individual_logits = {name: [] for name in list(image_models.keys()) + ['YellowFeatures']}

    with torch.no_grad():
        test_pbar = tqdm(test_loader, desc="æµ‹è¯•é›†è¯„ä¼°")

        for batch in test_pbar:
            images = batch['image'].to(device)
            yellow_features = batch['yellow_features'].to(device)
            labels = batch['label'].to(device)

            # è·å–æ‰€æœ‰æ¨¡å‹çš„logits
            logits_list = []
            model_names = list(image_models.keys()) + ['YellowFeatures']

            # å›¾åƒæ¨¡å‹logits
            for i, (model_name, model) in enumerate(image_models.items()):
                _, logits = model(images)
                logits_list.append(logits)
                individual_logits[model_name].append(logits.cpu())

            # é»„ç–¸ç‰¹å¾æ¨¡å‹logits
            yellow_logits = yellow_model(yellow_features)
            logits_list.append(yellow_logits)
            individual_logits['YellowFeatures'].append(yellow_logits.cpu())

            # é›†æˆèåˆ
            if config['ensemble']['fusion_method'] == 'mlp_with_yellow':
                fused_logits, attention_weights = ensemble_model(logits_list, yellow_features)
            else:
                fused_logits, attention_weights = ensemble_model(logits_list)

            # è·å–é¢„æµ‹ç»“æœ
            probabilities = torch.softmax(fused_logits, dim=1)
            _, predicted = torch.max(fused_logits, 1)

            # æ”¶é›†ç»“æœ
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probabilities.cpu().numpy())

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
    results = calculate_unified_metrics(all_labels, all_preds, all_probs, config['class_names'])

    # æ‰“å°ç»“æœ
    print(f"\n{'='*20} æµ‹è¯•é›†æœ€ç»ˆç»“æœ {'='*20}")
    print(f"å‡†ç¡®ç‡ (Accuracy): {results['accuracy']:.4f}")
    print(f"F1åˆ†æ•° (Macro): {results['f1_macro']:.4f}")
    print(f"F1åˆ†æ•° (Weighted): {results['f1_weighted']:.4f}")
    print(f"ç²¾ç¡®ç‡ (Macro): {results['precision_macro']:.4f}")
    print(f"å¬å›ç‡ (Macro): {results['recall_macro']:.4f}")
    print(f"æ•æ„Ÿåº¦ (å¹³å‡): {results['sensitivity_mean']:.4f}")
    print(f"ç‰¹å¼‚åº¦ (å¹³å‡): {results['specificity_mean']:.4f}")
    print(f"AUC: {results['auc']:.4f}")

    # æ¯ç±»åˆ«è¯¦ç»†ç»“æœ
    print(f"\næ¯ç±»åˆ«è¯¦ç»†ç»“æœ:")
    table = PrettyTable()
    table.field_names = ["ç±»åˆ«", "æ•æ„Ÿåº¦", "ç‰¹å¼‚åº¦"]
    for i, class_name in enumerate(config['class_names']):
        table.add_row([
            class_name,
            f"{results['sensitivity_per_class'][i]:.4f}",
            f"{results['specificity_per_class'][i]:.4f}"
        ])
    print(table)

    # æ··æ·†çŸ©é˜µ
    print(f"\næ··æ·†çŸ©é˜µ:")
    cm_table = PrettyTable()
    cm_table.field_names = ["å®é™…\\é¢„æµ‹"] + [f"Pred_{name}" for name in config['class_names']]
    for i, row in enumerate(results['confusion_matrix']):
        cm_table.add_row([f"True_{config['class_names'][i]}"] + list(row))
    print(cm_table)

    # åˆ†æå„æ¨¡å‹è´¡çŒ®åº¦
    analyze_model_contributions(individual_logits, all_labels, config)

    # ç»˜åˆ¶ROCæ›²çº¿
    roc_save_path = os.path.join(config['output_dir'], 'ensemble_roc_curve.svg')
    plot_multiclass_roc(all_labels, all_probs, config['class_names'], save_path=roc_save_path)

    return {
        'metrics': results,
        'predictions': (all_labels, all_preds, all_probs),
        'individual_logits': individual_logits
    }



def analyze_model_contributions(individual_logits, labels, config):
    """åˆ†æå„ä¸ªæ¨¡å‹çš„è´¡çŒ®åº¦"""

    print(f"\n{'='*20} æ¨¡å‹è´¡çŒ®åº¦åˆ†æ {'='*20}")

    model_performances = {}

    for model_name, logits_list in individual_logits.items():
        # åˆå¹¶æ‰€æœ‰logits
        all_logits = torch.cat(logits_list, dim=0)

        # è®¡ç®—å•ç‹¬é¢„æµ‹
        _, predictions = torch.max(all_logits, 1)
        predictions = predictions.numpy()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        accuracy = accuracy_score(labels, predictions)
        f1 = f1_score(labels, predictions, average='macro')

        model_performances[model_name] = {
            'accuracy': accuracy,
            'f1': f1
        }

    # æ‰“å°ç»“æœ
    print("å„æ¨¡å‹å•ç‹¬æ€§èƒ½:")
    perf_table = PrettyTable()
    perf_table.field_names = ["æ¨¡å‹", "å‡†ç¡®ç‡", "F1åˆ†æ•°"]

    for model_name, perf in model_performances.items():
        perf_table.add_row([
            model_name,
            f"{perf['accuracy']:.4f}",
            f"{perf['f1']:.4f}"
        ])

    print(perf_table)

    # ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    models = list(model_performances.keys())
    accuracies = [model_performances[m]['accuracy'] for m in models]
    f1_scores = [model_performances[m]['f1'] for m in models]

    # å‡†ç¡®ç‡å¯¹æ¯”
    bars1 = ax1.bar(models, accuracies, color=custom_palette[:len(models)])
    ax1.set_title('Individual Model Accuracy')
    ax1.set_ylabel('Accuracy')
    ax1.set_ylim(0, 1)
    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, acc in zip(bars1, accuracies):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{acc:.3f}', ha='center', va='bottom')

    # F1åˆ†æ•°å¯¹æ¯”
    bars2 = ax2.bar(models, f1_scores, color=custom_palette[:len(models)])
    ax2.set_title('Individual Model F1 Score')
    ax2.set_ylabel('F1 Score')
    ax2.set_ylim(0, 1)
    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, f1 in zip(bars2, f1_scores):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{f1:.3f}', ha='center', va='bottom')

    plt.tight_layout()
    save_path = os.path.join(config['output_dir'], 'individual_model_performance.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")


def generate_logits_fusion_report(image_models, yellow_model, ensemble_model,
                                 test_results, training_history, config):
    """ç”Ÿæˆlogitsèåˆé›†æˆçš„è¯¦ç»†æŠ¥å‘Š"""

    report_path = os.path.join(config['output_dir'], 'logits_fusion_ensemble_report.txt')

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Logitsèåˆé›†æˆå­¦ä¹ è¯¦ç»†æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")

        # 1. æ¶æ„ä¿¡æ¯
        f.write("1. æ¨¡å‹æ¶æ„\n")
        f.write("-" * 30 + "\n")
        f.write(f"åŸºç¡€å›¾åƒæ¨¡å‹æ•°é‡: {len(image_models)}\n")
        f.write(f"å›¾åƒæ¨¡å‹åˆ—è¡¨: {list(image_models.keys())}\n")
        f.write(f"é»„ç–¸ç‰¹å¾æ¨¡å‹: YellowFeatureClassifier (8ç»´è¾“å…¥)\n")
        f.write(f"èåˆæ–¹æ³•: {config['ensemble']['fusion_method']}\n")

        if config['ensemble']['fusion_method'] == 'self_attention':
            f.write(f"Self Attentionå‚æ•°:\n")
            f.write(f"  - d_model: {config['ensemble']['d_model']}\n")
            f.write(f"  - num_heads: {config['ensemble']['num_heads']}\n")
            f.write(f"  - dropout: {config['ensemble']['dropout']}\n")
            f.write(f"  - pooling_strategy: {config['ensemble']['pooling_strategy']}\n")

        f.write("\n")

        # 2. è®­ç»ƒé…ç½®
        f.write("2. è®­ç»ƒé…ç½®\n")
        f.write("-" * 30 + "\n")
        f.write(f"æ‰¹æ¬¡å¤§å°: {config['batch_size']}\n")
        f.write(f"å­¦ä¹ ç‡: {config['ensemble']['lr']}\n")
        f.write(f"æƒé‡è¡°å‡: {config['ensemble']['weight_decay']}\n")
        f.write(f"æœ€å¤§è®­ç»ƒè½®æ•°: {config['ensemble']['epochs']}\n")
        f.write(f"æ—©åœè€å¿ƒå€¼: {config['ensemble']['patience']}\n")
        f.write(f"å®é™…è®­ç»ƒè½®æ•°: {len(training_history['train_loss'])}\n")
        f.write("\n")

        # 3. æœ€ç»ˆæ€§èƒ½
        f.write("3. æµ‹è¯•é›†æœ€ç»ˆæ€§èƒ½\n")
        f.write("-" * 30 + "\n")
        metrics = test_results['metrics']
        f.write(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}\n")
        f.write(f"F1åˆ†æ•° (Macro): {metrics['f1_macro']:.4f}\n")
        f.write(f"F1åˆ†æ•° (Weighted): {metrics['f1_weighted']:.4f}\n")
        f.write(f"ç²¾ç¡®ç‡ (Macro): {metrics['precision_macro']:.4f}\n")
        f.write(f"å¬å›ç‡ (Macro): {metrics['recall_macro']:.4f}\n")
        f.write(f"æ•æ„Ÿåº¦ (å¹³å‡): {metrics['sensitivity_mean']:.4f}\n")
        f.write(f"ç‰¹å¼‚åº¦ (å¹³å‡): {metrics['specificity_mean']:.4f}\n")
        f.write(f"AUC: {metrics['auc']:.4f}\n")
        f.write("\n")

        # 4. æ¯ç±»åˆ«æ€§èƒ½
        f.write("4. æ¯ç±»åˆ«è¯¦ç»†æ€§èƒ½\n")
        f.write("-" * 30 + "\n")
        for i, class_name in enumerate(config['class_names']):
            f.write(f"{class_name}:\n")
            f.write(f"  æ•æ„Ÿåº¦: {metrics['sensitivity_per_class'][i]:.4f}\n")
            f.write(f"  ç‰¹å¼‚åº¦: {metrics['specificity_per_class'][i]:.4f}\n")
        f.write("\n")

        # 5. è®­ç»ƒå†å²ç»Ÿè®¡
        f.write("5. è®­ç»ƒå†å²ç»Ÿè®¡\n")
        f.write("-" * 30 + "\n")
        f.write(f"æœ€ä½³è®­ç»ƒF1: {max(training_history['train_f1']):.4f}\n")
        f.write(f"æœ€ä½³éªŒè¯F1: {max(training_history['val_f1']):.4f}\n")
        f.write(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_history['train_loss'][-1]:.4f}\n")
        f.write(f"æœ€ç»ˆéªŒè¯æŸå¤±: {training_history['val_loss'][-1]:.4f}\n")

        # æ£€æŸ¥è¿‡æ‹Ÿåˆ
        final_gap = training_history['train_f1'][-1] - training_history['val_f1'][-1]
        f.write(f"æœ€ç»ˆè®­ç»ƒ-éªŒè¯F1å·®è·: {final_gap:.4f}\n")
        if final_gap > 0.1:
            f.write("âš ï¸  æ£€æµ‹åˆ°è½»å¾®è¿‡æ‹Ÿåˆ\n")
        f.write("\n")

        # 6. æ¨¡å‹æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if config['ensemble']['fusion_method'] == 'weighted_average':
            f.write("6. æ¨¡å‹æƒé‡åˆ†é…\n")
            f.write("-" * 30 + "\n")
            weights = torch.softmax(ensemble_model.model_weights, dim=0).cpu().numpy()
            model_names = list(image_models.keys()) + ['YellowFeatures']
            for name, weight in zip(model_names, weights):
                f.write(f"{name}: {weight:.4f}\n")
            f.write("\n")

        # 7. æŠ€æœ¯ç‰¹ç‚¹
        f.write("7. æŠ€æœ¯ç‰¹ç‚¹\n")
        f.write("-" * 30 + "\n")
        f.write("âœ“ ç›´æ¥å¯¹logitsè¿›è¡Œèåˆï¼Œä¿ç•™æ›´å¤šä¿¡æ¯\n")
        f.write("âœ“ ä½¿ç”¨æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ\n")
        f.write("âœ“ çœŸæ­£çš„Self Attentionæœºåˆ¶å­¦ä¹ æ¨¡å‹é—´å…³ç³»\n")
        f.write("âœ“ å¤šç§èåˆç­–ç•¥å¯é€‰\n")
        f.write("âœ“ è¯¦ç»†çš„æ³¨æ„åŠ›æƒé‡åˆ†æ\n")
        f.write("âœ“ ä¸ªä½“æ¨¡å‹è´¡çŒ®åº¦åˆ†æ\n")

    print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

def calc_sensitivity_bootstrap(y_true, y_pred, y_prob):
    """è®¡ç®—æ•æ„Ÿåº¦çš„Bootstrapå‡½æ•°"""
    cm = confusion_matrix(y_true, y_pred)
    sensitivities = np.diag(cm) / np.sum(cm, axis=1)
    return np.mean(sensitivities)


def calc_specificity_bootstrap(y_true, y_pred, y_prob):
    """è®¡ç®—ç‰¹å¼‚åº¦çš„Bootstrapå‡½æ•°"""
    cm = confusion_matrix(y_true, y_pred)
    n_classes = cm.shape[0]
    specificities = []
    for i in range(n_classes):
        TP = cm[i, i]
        FN = cm[i, :].sum() - TP
        FP = cm[:, i].sum() - TP
        TN = cm.sum() - (TP + FP + FN)
        spec = TN / (TN + FP) if (TN + FP) > 0 else 0
        specificities.append(spec)
    return np.mean(specificities)


def calc_auc_bootstrap(y_true, y_pred, y_prob):
    """è®¡ç®—AUCçš„Bootstrapå‡½æ•°"""
    from sklearn.preprocessing import label_binarize
    y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
    return roc_auc_score(y_true_bin, y_prob, average='macro', multi_class='ovr')


def calculate_unified_metrics(y_true, y_pred, y_prob, class_names):
    """è®¡ç®—ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆåŒ…å«APï¼‰"""
    from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                                 f1_score, confusion_matrix, roc_auc_score,
                                 average_precision_score)
    from sklearn.preprocessing import label_binarize

    # åŸºç¡€æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    f1_macro = f1_score(y_true, y_pred, average='macro')
    f1_weighted = f1_score(y_true, y_pred, average='weighted')
    precision_macro = precision_score(y_true, y_pred, average='macro')
    recall_macro = recall_score(y_true, y_pred, average='macro')

    # æ··æ·†çŸ©é˜µå’Œæ•æ„Ÿåº¦/ç‰¹å¼‚åº¦
    cm = confusion_matrix(y_true, y_pred)
    n_classes = len(class_names)

    sensitivity = []
    specificity = []
    for i in range(n_classes):
        TP = cm[i, i]
        FN = cm[i, :].sum() - TP
        FP = cm[:, i].sum() - TP
        TN = cm.sum() - (TP + FP + FN)

        sens = TP / (TP + FN) if (TP + FN) > 0 else 0
        spec = TN / (TN + FP) if (TN + FP) > 0 else 0

        sensitivity.append(sens)
        specificity.append(spec)

    # AUCè®¡ç®—
    try:
        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))
        if n_classes > 2:
            auc_score = roc_auc_score(y_true_bin, y_prob, multi_class='ovr', average='macro')
        else:
            auc_score = roc_auc_score(y_true, y_prob[:, 1])
    except:
        auc_score = 0.0

    # APè®¡ç®—ï¼ˆæ–°å¢ï¼‰
    try:
        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))
        if n_classes > 2:
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„AP
            class_aps = []
            for i in range(n_classes):
                ap = average_precision_score(y_true_bin[:, i], y_prob[:, i])
                class_aps.append(ap)
            ap_macro = np.mean(class_aps)
            ap_weighted = average_precision_score(y_true_bin, y_prob, average='weighted')
        else:
            ap_macro = average_precision_score(y_true, y_prob[:, 1])
            ap_weighted = ap_macro
            class_aps = [ap_macro]
    except:
        ap_macro = 0.0
        ap_weighted = 0.0
        class_aps = [0.0] * n_classes

    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'sensitivity_mean': np.mean(sensitivity),
        'specificity_mean': np.mean(specificity),
        'auc': auc_score,
        'ap_macro': ap_macro,  # æ–°å¢
        'ap_weighted': ap_weighted,  # æ–°å¢
        'ap_per_class': class_aps,  # æ–°å¢
        'sensitivity_per_class': sensitivity,
        'specificity_per_class': specificity,
        'confusion_matrix': cm
    }


def unified_evaluate_all_models(models, testloaders, ensemble_results, device, class_names):
    """ç»Ÿä¸€è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§"""

    all_results = {}

    # 1. è¯„ä¼°å•ä¸ªæ¨¡å‹
    for model_name, model in models.items():
        print(f"Unified evaluation for {model_name}...")
        model.eval()
        all_preds = []
        all_labels = []
        all_probs = []

        with torch.no_grad():
            for batch in testloaders[model_name]:
                if len(batch) == 4:  # åŒ…å«yellow features
                    inputs, _, labels, _ = batch
                else:  # ä¸åŒ…å«yellow features
                    inputs, labels, _ = batch

                inputs = inputs.to(device)
                labels = labels.to(device)

                _, outputs = model(inputs)
                probs = F.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        y_true = np.array(all_labels)
        y_pred = np.array(all_preds)
        y_prob = np.array(all_probs)

        # è®¡ç®—ç»Ÿä¸€çš„æŒ‡æ ‡
        metrics = calculate_unified_metrics(y_true, y_pred, y_prob, class_names)
        all_results[model_name] = {
            'metrics': metrics,
            'predictions': (y_true, y_pred, y_prob)
        }

    # 2. æ·»åŠ Ensembleç»“æœ
    if ensemble_results is not None:
        y_test, test_pred, test_prob = ensemble_results
        metrics = calculate_unified_metrics(y_test, test_pred, test_prob, class_names)
        all_results['Ensemble'] = {
            'metrics': metrics,
            'predictions': (y_test, test_pred, test_prob)
        }

    return all_results


def create_unified_bootstrap_chart_corrected(unified_results, config):
    """ä¿®æ­£ç‰ˆï¼šæ˜ç¡®ä½¿ç”¨Bootstrapç»“æœï¼Œæ ‡é¢˜å’Œæ ‡ç­¾éƒ½è¯´æ˜è¿™ä¸€ç‚¹"""

    metrics = ['Accuracy', 'F1', 'Sensitivity', 'Specificity', 'AUC']
    models = list(unified_results.keys())

    # è®¡ç®—Bootstrapç½®ä¿¡åŒºé—´
    bootstrap_results = {}

    for model_name, result in unified_results.items():
        y_true, y_pred, y_prob = result['predictions']
        bootstrap_results[model_name] = {}

        for metric in metrics:
            if metric == 'Accuracy':
                mean, (lower, upper) = bootstrap_metric(
                    y_true, y_pred, None,
                    lambda y_true, y_pred, *args: accuracy_score(y_true, y_pred)
                )
            elif metric == 'F1':
                mean, (lower, upper) = bootstrap_metric(
                    y_true, y_pred, None,
                    lambda y_true, y_pred, *args: f1_score(y_true, y_pred, average='macro')
                )
            elif metric == 'Sensitivity':
                mean, (lower, upper) = bootstrap_metric(
                    y_true, y_pred, None, calc_sensitivity_bootstrap
                )
            elif metric == 'Specificity':
                mean, (lower, upper) = bootstrap_metric(
                    y_true, y_pred, None, calc_specificity_bootstrap
                )
            elif metric == 'AUC':
                mean, (lower, upper) = bootstrap_metric(
                    y_true, y_pred, y_prob, calc_auc_bootstrap
                )

            bootstrap_results[model_name][metric] = (mean, lower, upper)

    # ç»˜åˆ¶æŸ±çŠ¶å›¾
    plt.figure(figsize=(12, 8))
    bar_width = 0.15
    index = np.arange(len(models))
    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974']

    for i, metric in enumerate(metrics):
        means = []
        lowers = []
        uppers = []

        for model_name in models:
            mean, lower, upper = bootstrap_results[model_name][metric]
            means.append(mean)
            lowers.append(lower)
            uppers.append(upper)

        means = np.array(means)
        lowers = np.array(lowers)
        uppers = np.array(uppers)
        yerr = np.vstack([means - lowers, uppers - means])

        plt.bar(
            index + i * bar_width,
            means,  # â† è¿™æ˜¯Bootstrapå‡å€¼
            bar_width,
            label=metric,
            color=colors[i % len(colors)],
            yerr=yerr,  # â† è¿™æ˜¯Bootstrapç½®ä¿¡åŒºé—´
            capsize=3,
            edgecolor='black',
            linewidth=0.8,
            error_kw={'elinewidth': 1.0, 'capthick': 1.0}
        )

        # æ·»åŠ æ•°å€¼æ ‡ç­¾ - æ˜ç¡®æ˜¾ç¤ºè¿™æ˜¯Bootstrapå‡å€¼
        for j, value in enumerate(means):
            plt.text(
                j + i * bar_width,
                uppers[j] + 0.01,
                f'{value:.3f}',  # â† Bootstrapå‡å€¼
                ha='center',
                va='bottom',
                fontsize=8,
                rotation=90
            )

    plt.xlabel('Models', fontsize=12)
    plt.ylabel('Bootstrap Mean Â± 95% CI', fontsize=12)  # â† æ˜ç¡®æ ‡æ³¨
    plt.title('Model Comparison - Bootstrap Estimates (500 iterations)', fontsize=14)  # â† æ˜ç¡®æ ‡æ³¨
    plt.xticks(index + bar_width * (len(metrics) - 1) / 2, models, rotation=15)
    plt.ylim(0, 1.05)
    plt.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, -0.08), ncol=5)
    plt.tight_layout(rect=[0, 0.07, 1, 1])

    save_path = os.path.join(config['output_dir'], 'bootstrap_model_comparison.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"BootstrapæŸ±çŠ¶å›¾å·²ä¿å­˜åˆ°: {save_path}")
    print("ğŸ“Š å›¾è¡¨æ˜¾ç¤ºçš„æ˜¯Bootstrapä¼°è®¡å€¼ï¼ˆ500æ¬¡é‡é‡‡æ ·çš„å‡å€¼ï¼‰åŠå…¶95%ç½®ä¿¡åŒºé—´")

    return bootstrap_results  # è¿”å›Bootstrapç»“æœç”¨äºè¡¨æ ¼


def train_single_model(model, train_loader, val_loader, device, epochs=10,
                       lr=1e-4, weight_decay=1e-4, patience=5, model_path=None):
    """Train a single backbone model"""

    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)

    # Get class weights for handling imbalance
    all_train_labels = []
    for batch in train_loader:
        if len(batch) == 3:  # No yellow features
            _, labels, _ = batch
        else:  # With yellow features
            _, _, labels, _ = batch
        all_train_labels.extend(labels.numpy())

    class_weights = compute_class_weights(np.array(all_train_labels)).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # Early stopping
    best_val_f1 = 0
    no_improve_epochs = 0

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} Training"):
            if len(batch) == 3:  # No yellow features
                inputs, labels, _ = batch
            else:  # With yellow features
                inputs, _, labels, _ = batch

            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            _, outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            train_correct += predicted.eq(labels).sum().item()
            train_total += labels.size(0)

        train_loss = train_loss / train_total
        train_acc = train_correct / train_total

        # Validation phase
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        val_preds = []
        val_labels_list = []

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{epochs} Validation"):
                if len(batch) == 3:  # No yellow features
                    inputs, labels, _ = batch
                else:  # With yellow features
                    inputs, _, labels, _ = batch

                inputs, labels = inputs.to(device), labels.to(device)

                _, outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * inputs.size(0)
                _, predicted = outputs.max(1)
                val_correct += predicted.eq(labels).sum().item()
                val_total += labels.size(0)

                val_preds.extend(predicted.cpu().numpy())
                val_labels_list.extend(labels.cpu().numpy())

        val_loss = val_loss / val_total
        val_acc = val_correct / val_total
        val_f1 = f1_score(val_labels_list, val_preds, average='macro')

        # Update scheduler
        scheduler.step(val_f1)

        # Print progress
        print(f"Epoch {epoch + 1}/{epochs}")
        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")

        # Check for improvement
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            no_improve_epochs = 0

            # Save model
            if model_path:
                torch.save(model.state_dict(), model_path)
                print(f"Model saved to {model_path}")
        else:
            no_improve_epochs += 1
            print(f"No improvement for {no_improve_epochs} epochs")

        # Early stopping
        if no_improve_epochs >= patience:
            print("Early stopping triggered")
            break

    # Load best model if saved
    if model_path and os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path))

    return model


def save_comprehensive_results(unified_results, config):
    """ä¿å­˜ç»¼åˆç»“æœåˆ°JSONæ–‡ä»¶"""

    # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    serializable_results = {}

    for model_name, result in unified_results.items():
        metrics = result['metrics']
        y_true, y_pred, y_prob = result['predictions']

        serializable_results[model_name] = {
            'metrics': {
                'accuracy': float(metrics['accuracy']),
                'f1_macro': float(metrics['f1_macro']),
                'f1_weighted': float(metrics['f1_weighted']),
                'precision_macro': float(metrics['precision_macro']),
                'recall_macro': float(metrics['recall_macro']),
                'sensitivity_mean': float(metrics['sensitivity_mean']),
                'specificity_mean': float(metrics['specificity_mean']),
                'auc': float(metrics['auc']),
                'sensitivity_per_class': [float(x) for x in metrics['sensitivity_per_class']],
                'specificity_per_class': [float(x) for x in metrics['specificity_per_class']],
                'confusion_matrix': metrics['confusion_matrix'].tolist()
            },
            'predictions': {
                'y_true': y_true.tolist(),
                'y_pred': y_pred.tolist(),
                'y_prob': y_prob.tolist()
            }
        }

    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    results_path = os.path.join(config['output_dir'], 'comprehensive_results.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=4, ensure_ascii=False)

    print(f"ç»¼åˆç»“æœå·²ä¿å­˜åˆ°: {results_path}")



def extract_features(model, data_loader, device, include_yellow=True):
    """Extract features from a trained model for all samples in the data loader"""
    model.eval()
    features_list = []
    yellow_list = []
    labels_list = []
    serials_list = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Extracting features"):
            if include_yellow:
                inputs, yellow, labels, serials = batch
                inputs = inputs.to(device)
                yellow = yellow.to(device)

                # Extract features from backbone
                features, _ = model(inputs)

                # Store features and metadata
                features_list.append(features.cpu())
                yellow_list.append(yellow.cpu())
                labels_list.append(labels)
                serials_list.extend(serials)
            else:
                inputs, labels, serials = batch
                inputs = inputs.to(device)

                # Extract features from backbone
                features, _ = model(inputs)

                # Store features and metadata
                features_list.append(features.cpu())
                labels_list.append(labels)
                serials_list.extend(serials)

    # Concatenate all features
    all_features = torch.cat(features_list, dim=0)
    all_labels = torch.cat(labels_list, dim=0)

    if include_yellow:
        all_yellow = torch.cat(yellow_list, dim=0)
        return all_features, all_yellow, all_labels, serials_list
    else:
        return all_features, all_labels, serials_list


def extract_logits(model, data_loader, device, include_yellow=True):
    model.eval()
    logits_list = []
    yellow_list = []
    labels_list = []
    serials_list = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Extracting logits"):
            if include_yellow:
                inputs, yellow, labels, serials = batch
                inputs = inputs.to(device)
                yellow = yellow.to(device)
                _, logits = model(inputs)
                logits_list.append(logits.cpu())
                yellow_list.append(yellow.cpu())
                labels_list.append(labels)
                serials_list.extend(serials)
            else:
                inputs, labels, serials = batch
                inputs = inputs.to(device)
                _, logits = model(inputs)
                logits_list.append(logits.cpu())
                labels_list.append(labels)
                serials_list.extend(serials)

    all_logits = torch.cat(logits_list, dim=0)
    all_labels = torch.cat(labels_list, dim=0)
    if include_yellow:
        all_yellow = torch.cat(yellow_list, dim=0)
        return all_logits, all_yellow, all_labels, serials_list
    else:
        return all_logits, all_labels, serials_list


def train_ensemble_model(ensemble_model, feature_loaders, train_data, val_data, device,
                         epochs=20, lr=1e-4, weight_decay=1e-4,
                         patience=7, model_path=None, include_yellow=True):

    ensemble_model.to(device)
    optimizer = torch.optim.AdamW(ensemble_model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)

    # unpack train data
    if include_yellow:
        train_features, train_yellow, train_labels, _ = train_data
        val_features, val_yellow, val_labels, _ = val_data
    else:
        train_features, train_labels, _ = train_data
        val_features, val_labels, _ = val_data

    class_weights = compute_class_weights(train_labels.numpy()).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    best_val_f1 = 0
    no_improve_epochs = 0

    for epoch in range(epochs):
        ensemble_model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        train_preds = []
        train_labels_list = []

        indices = torch.randperm(len(train_labels))
        batch_size = 32

        for i in range(0, len(train_labels), batch_size):
            batch_indices = indices[i:i+batch_size]
            batch_features = [feat[batch_indices].to(device) for feat in train_features]
            batch_labels = train_labels[batch_indices].to(device)
            if include_yellow:
                batch_yellow = train_yellow[batch_indices].to(device)
                outputs = ensemble_model(batch_features, batch_yellow)
            else:
                outputs = ensemble_model(batch_features)
            loss = criterion(outputs, batch_labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * batch_labels.size(0)
            _, predicted = outputs.max(1)
            train_correct += predicted.eq(batch_labels).sum().item()
            train_total += batch_labels.size(0)
            train_preds.extend(predicted.cpu().numpy())
            train_labels_list.extend(batch_labels.cpu().numpy())

        train_loss = train_loss / train_total if train_total > 0 else 0
        train_acc = train_correct / train_total if train_total > 0 else 0
        train_f1 = f1_score(train_labels_list, train_preds, average='macro')

        # ----------- éªŒè¯é˜¶æ®µï¼Œç”¨val_data -----------
        ensemble_model.eval()
        with torch.no_grad():
            val_batch_features = [feat.to(device) for feat in val_features]
            val_batch_labels = val_labels.to(device)
            if include_yellow:
                val_batch_yellow = val_yellow.to(device)
                outputs = ensemble_model(val_batch_features, val_batch_yellow)
            else:
                outputs = ensemble_model(val_batch_features)
            val_loss = criterion(outputs, val_batch_labels).item()
            _, predicted = outputs.max(1)
            val_correct = predicted.eq(val_batch_labels).sum().item()
            val_total = val_batch_labels.size(0)
            val_preds = predicted.cpu().numpy()
            val_labels_list = val_batch_labels.cpu().numpy()
            val_acc = val_correct / val_total if val_total > 0 else 0
            val_f1 = f1_score(val_labels_list, val_preds, average='macro')

        scheduler.step(val_f1)

        print(f"Epoch {epoch + 1}/{epochs}")
        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")

        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            no_improve_epochs = 0
            if model_path:
                torch.save(ensemble_model.state_dict(), model_path)
                print(f"Ensemble model saved to {model_path}")
        else:
            no_improve_epochs += 1
            print(f"No improvement for {no_improve_epochs} epochs")

        if no_improve_epochs >= patience:
            print("Early stopping triggered")
            break

    if model_path and os.path.exists(model_path):
        ensemble_model.load_state_dict(torch.load(model_path))

    return ensemble_model



def evaluate_model(model, test_data, device, class_names=None, is_ensemble=False, feature_loaders=None,
                   include_yellow=True,roc_save_path=None):
    """Evaluate model on test set with detailed metrics"""

    if class_names is None:
        class_names = ['Mild', 'Moderate', 'Severe']

    if is_ensemble and feature_loaders is None:
        raise ValueError("Must provide feature_loaders for ensemble evaluation")

    # Set model to evaluation mode
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        if is_ensemble:
            # For ensemble model, use pre-extracted features
            if include_yellow:
                # Unpack test data
                _, yellow_features, labels, _ = test_data

                # Move features to device
                test_features = []
                for feat in feature_loaders:
                    test_features.append(feat.to(device))

                test_yellow = yellow_features.to(device)
                test_labels = labels.to(device)

                # Forward pass
                outputs = model(test_features, test_yellow)
            else:
                # Unpack test data
                _, labels, _ = test_data

                # Move features to device
                test_features = []
                for feat in feature_loaders:
                    test_features.append(feat.to(device))

                test_labels = labels.to(device)

                # Forward pass
                outputs = model(test_features)

            # Get predictions
            probs = F.softmax(outputs, dim=1)
            preds = torch.argmax(outputs, dim=1)

            # Collect results
            all_preds = preds.cpu().numpy()
            all_labels = test_labels.cpu().numpy()
            all_probs = probs.cpu().numpy()

        else:
            # For single model, iterate through test loader
            for batch in tqdm(test_data, desc="Evaluating"):
                if include_yellow:
                    inputs, yellow, labels, _ = batch
                    inputs = inputs.to(device)
                else:
                    inputs, labels, _ = batch
                    inputs = inputs.to(device)

                labels = labels.to(device)

                # Forward pass
                if hasattr(model, 'forward') and callable(getattr(model, 'forward')):
                    _, outputs = model(inputs)
                else:
                    outputs = model(inputs)

                # Get predictions
                probs = F.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)

                # Collect results
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())

    # Convert to numpy arrays if needed
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # Calculate metrics
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='weighted')
    recall = recall_score(all_labels, all_preds, average='weighted')
    f1 = f1_score(all_labels, all_preds, average='weighted')
    conf_matrix = confusion_matrix(all_labels, all_preds)
    # è®¡ç®—æ¯ç±»çš„æ•æ„Ÿåº¦ï¼ˆRecallï¼‰ã€ç‰¹å¼‚åº¦ï¼ˆSpecificityï¼‰
    num_classes = len(class_names)
    sensitivity = []
    specificity = []
    for i in range(num_classes):
        TP = conf_matrix[i, i]
        FN = conf_matrix[i, :].sum() - TP
        FP = conf_matrix[:, i].sum() - TP
        TN = conf_matrix.sum() - (TP + FP + FN)
        sens = TP / (TP + FN) if (TP + FN) > 0 else 0
        spec = TN / (TN + FP) if (TN + FP) > 0 else 0
        sensitivity.append(sens)
        specificity.append(spec)

    # æ‰“å°
    print("\nPer-class Sensitivity (Recall) & Specificity:")
    table2 = PrettyTable()
    table2.field_names = ["Class", "Sensitivity (Recall)", "Specificity"]
    for i, class_name in enumerate(class_names):
        table2.add_row([class_name, f"{sensitivity[i]:.4f}", f"{specificity[i]:.4f}"])
    print(table2)

    # Calculate ROC curve and AUC
    try:
        from sklearn.metrics import roc_auc_score
        from sklearn.preprocessing import label_binarize

        # Binarize labels for multi-class ROC
        classes = np.unique(all_labels)
        y_bin = label_binarize(all_labels, classes=classes)

        if len(classes) > 2:
            roc_auc = roc_auc_score(y_bin, all_probs, multi_class='ovr', average='weighted')
        else:
            roc_auc = roc_auc_score(all_labels, all_probs[:, 1])
    except Exception as e:
        print(f"Warning: Could not calculate ROC AUC: {e}")
        roc_auc = 0

    # Print results
    print(f"===== Test Set Evaluation =====")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    print("\nConfusion Matrix:")

    # Pretty print confusion matrix
    table = PrettyTable()
    table.field_names = [""] + [f"Pred {name}" for name in class_names]
    for i, row in enumerate(conf_matrix):
        table.add_row([f"True {class_names[i]}"] + list(row))
    print(table)

    # Per-class metrics
    precision_per_class = precision_score(all_labels, all_preds, average=None)
    recall_per_class = recall_score(all_labels, all_preds, average=None)
    f1_per_class = f1_score(all_labels, all_preds, average=None)

    print("\nPer-class Metrics:")
    table = PrettyTable()
    table.field_names = ["Class", "Precision", "Recall", "F1 Score"]
    for i, class_name in enumerate(class_names):
        if i < len(precision_per_class):
            table.add_row([class_name, f"{precision_per_class[i]:.4f}",
                           f"{recall_per_class[i]:.4f}", f"{f1_per_class[i]:.4f}"])
    print(table)
    if roc_save_path is None:
        roc_save_path = "roc_curve.png"
    plot_multiclass_roc(all_labels, all_probs, class_names, save_path=roc_save_path)

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "roc_auc": roc_auc,
        "confusion_matrix": conf_matrix,
        "precision_per_class": precision_per_class,
        "recall_per_class": recall_per_class,
        "f1_per_class": f1_per_class,
        "sensitivity_per_class": sensitivity,
        "specificity_per_class": specificity,
        "all_labels": all_labels,
        "all_probs": all_probs
    }


# ========== 7. Main Training Pipeline ==========
def unified_evaluate_all_models(image_models, yellow_model, fusion_model, test_loader, device, config):
    """ç»Ÿä¸€è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§"""

    all_results = {}

    # 1. è¯„ä¼°å•ä¸ªå›¾åƒæ¨¡å‹
    for model_name, model in image_models.items():
        print(f"Unified evaluation for {model_name}...")
        model.eval()
        all_preds = []
        all_labels = []
        all_probs = []

        with torch.no_grad():
            for batch in test_loader:
                images = batch['image'].to(device)
                labels = batch['label'].to(device)

                _, outputs = model(images)
                probs = F.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        y_true = np.array(all_labels)
        y_pred = np.array(all_preds)
        y_prob = np.array(all_probs)

        # è®¡ç®—ç»Ÿä¸€çš„æŒ‡æ ‡
        metrics = calculate_unified_metrics(y_true, y_pred, y_prob, config['class_names'])
        all_results[model_name] = {
            'metrics': metrics,
            'predictions': (y_true, y_pred, y_prob)
        }

    # 2. è¯„ä¼°é»„ç–¸ç‰¹å¾æ¨¡å‹
    print("Unified evaluation for Yellow Features...")
    yellow_model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in test_loader:
            yellow_features = batch['yellow_features'].to(device)
            labels = batch['label'].to(device)

            outputs = yellow_model(yellow_features)
            probs = F.softmax(outputs, dim=1)
            preds = torch.argmax(outputs, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    y_true = np.array(all_labels)
    y_pred = np.array(all_preds)
    y_prob = np.array(all_probs)

    metrics = calculate_unified_metrics(y_true, y_pred, y_prob, config['class_names'])
    all_results['YellowFeatures'] = {
        'metrics': metrics,
        'predictions': (y_true, y_pred, y_prob)
    }

    # 3. è¯„ä¼°Dynamic Weight Fusion
    print("Unified evaluation for Dynamic Weight Fusion...")
    fusion_model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in test_loader:
            images = batch['image'].to(device)
            yellow_features = batch['yellow_features'].to(device)
            labels = batch['label'].to(device)

            # è·å–æ‰€æœ‰æ¨¡å‹çš„logits
            logits_list = []

            # å›¾åƒæ¨¡å‹logits
            for model in image_models.values():
                _, logits = model(images)
                logits_list.append(logits)

            # é»„ç–¸æ¨¡å‹logits
            yellow_logits = yellow_model(yellow_features)
            logits_list.append(yellow_logits)

            # å †å logits
            stacked_logits = torch.stack(logits_list, dim=1)

            # Dynamic fusion
            fused_output, weights = fusion_model(stacked_logits)
            probs = F.softmax(fused_output, dim=1)
            preds = fused_output.argmax(dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    y_true = np.array(all_labels)
    y_pred = np.array(all_preds)
    y_prob = np.array(all_probs)

    metrics = calculate_unified_metrics(y_true, y_pred, y_prob, config['class_names'])
    all_results['DynamicFusion'] = {
        'metrics': metrics,
        'predictions': (y_true, y_pred, y_prob)
    }

    return all_results

def plot_comprehensive_roc(unified_results, config):
    """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„ç»¼åˆROCæ›²çº¿"""

    plt.figure(figsize=(10, 8))
    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#64B5CD']

    for i, (model_name, result) in enumerate(unified_results.items()):
        y_true, y_pred, y_prob = result['predictions']

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROC
        n_classes = len(config['class_names'])
        y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))

        # è®¡ç®—macro-average ROC AUC
        fpr = dict()
        tpr = dict()
        roc_auc = dict()

        for class_idx in range(n_classes):
            fpr[class_idx], tpr[class_idx], _ = roc_curve(y_true_bin[:, class_idx], y_prob[:, class_idx])
            roc_auc[class_idx] = auc(fpr[class_idx], tpr[class_idx])

        # è®¡ç®—macro-average ROC
        all_fpr = np.unique(np.concatenate([fpr[j] for j in range(n_classes)]))
        mean_tpr = np.zeros_like(all_fpr)
        for class_idx in range(n_classes):
            mean_tpr += np.interp(all_fpr, fpr[class_idx], tpr[class_idx])
        mean_tpr /= n_classes

        macro_auc = auc(all_fpr, mean_tpr)

        plt.plot(all_fpr, mean_tpr,
                 label=f'{model_name} (AUC = {macro_auc:.3f})',
                 color=colors[i % len(colors)], linewidth=2)

    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Comprehensive ROC Curves - All Models')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)

    save_path = os.path.join(config['output_dir'], 'comprehensive_roc_curves.svg')
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()

    print(f"ç»¼åˆROCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")


def train_and_evaluate_ensemble_layered(config):
    """æ–¹æ¡ˆCï¼šåˆ†å±‚é›†æˆè®­ç»ƒå’Œè¯„ä¼°ï¼ˆå¤ç”¨å·²æœ‰å›¾åƒæ¨¡å‹ï¼‰"""
    import json

    set_seed(config['seed'])
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    os.makedirs(config['output_dir'], exist_ok=True)
    train_dir = os.path.join(config['dataset_dir'], "train")
    val_dir = os.path.join(config['dataset_dir'], "val")
    test_dir = os.path.join(config['dataset_dir'], "test")

    train_names = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]
    val_names = [d for d in os.listdir(val_dir) if os.path.isdir(os.path.join(val_dir, d))]
    test_names = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]
    print(f"Train: {len(train_names)}, Val: {len(val_names)}, Test: {len(test_names)}")

    # =================== ç¬¬ä¸€å±‚ï¼šåŠ è½½å·²æœ‰çš„å›¾åƒæ¨¡å‹ ===================
    print("=" * 50)
    print("ç¬¬ä¸€å±‚ï¼šåŠ è½½å·²æœ‰çš„å›¾åƒæ¨¡å‹")
    print("=" * 50)

    transforms = {
        'convnext': T.Compose([
            T.Resize((224, 224)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        'swin': T.Compose([
            T.Resize((224, 224)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        'efficientnet': T.Compose([
            T.Resize((384, 384)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        'vit': T.Compose([
            T.Resize((224, 224)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    }

    # åŠ è½½å·²æœ‰çš„å›¾åƒæ¨¡å‹
    image_models = {}
    image_loaders = {}  # ç”¨äºæµ‹è¯•è¯„ä¼°

    for model_name in config['models']:
        print(f"\nåŠ è½½ {model_name} æ¨¡å‹...")

        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        if model_name == 'ConvNext':
            image_models[model_name] = ConvNextBackbone(
                model_name='ConvNext', num_classes=3, pretrained=True, freeze_base=config['freeze_base']
            )
        elif model_name == 'Swin':
            image_models[model_name] = SwinTransformerBackbone(
                model_name='Swin', num_classes=3, pretrained=True, freeze_base=config['freeze_base']
            )
        elif model_name == 'EfficientNet':
            image_models[model_name] = EfficientNetBackbone(
                model_name='EfficientNet', num_classes=3, pretrained=True, freeze_base=config['freeze_base']
            )
        elif model_name == 'ViT':
            image_models[model_name] = ViTBackbone(
                model_name='ViT', num_classes=3, pretrained=True, freeze_base=config['freeze_base']
            )

        # åŠ è½½å·²è®­ç»ƒçš„æƒé‡
        if config['reuse_image_models'] and model_name in config['existing_model_paths']:
            model_path = config['existing_model_paths'][model_name]
            if os.path.exists(model_path):
                print(f"ä» {model_path} åŠ è½½å·²è®­ç»ƒçš„ {model_name} æ¨¡å‹")
                image_models[model_name].load_state_dict(torch.load(model_path, map_location=device))
                image_models[model_name].to(device)
                image_models[model_name].eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                print(f"âœ“ {model_name} æ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                print(f"âŒ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {model_name} çš„æ¨¡å‹æ–‡ä»¶ {model_path}")
                print(f"å°†é‡æ–°è®­ç»ƒ {model_name} æ¨¡å‹...")
                # è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ–°è®­ç»ƒçš„é€»è¾‘ï¼Œæˆ–è€…æŠ›å‡ºå¼‚å¸¸
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        else:
            print(f"é…ç½®ä¸ºé‡æ–°è®­ç»ƒ {model_name} æ¨¡å‹")
            # å¦‚æœéœ€è¦é‡æ–°è®­ç»ƒï¼Œè¿™é‡Œæ·»åŠ è®­ç»ƒé€»è¾‘

        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆç”¨äºåç»­è¯„ä¼°ï¼‰
        test_dataset = Jaundice3ClassDataset_ImageOnly(
            test_dir, test_names, split='test',
            bilirubin_csv=config['bilirubin_csv'],
            transform=transforms[model_name.lower()]
        )
        image_loaders[f'{model_name}_test'] = DataLoader(
            test_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=config['num_workers']
        )

    print(f"\nâœ“ æˆåŠŸåŠ è½½ {len(image_models)} ä¸ªå›¾åƒæ¨¡å‹")

    # =================== ç¬¬äºŒå±‚ï¼šè®­ç»ƒé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ ===================
    print("\n" + "=" * 50)
    print("ç¬¬äºŒå±‚ï¼šè®­ç»ƒé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ï¼ˆåªä½¿ç”¨é»„ç–¸ç‰¹å¾ï¼‰")
    print("=" * 50)

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„é»„ç–¸åˆ†ç±»å™¨
    yellow_model_path = os.path.join(config['output_dir'], 'best_yellow_classifier.pth')

    if os.path.exists(yellow_model_path) and config['use_pretrained']:
        print(f"å‘ç°å·²è®­ç»ƒçš„é»„ç–¸åˆ†ç±»å™¨: {yellow_model_path}")
        print("åŠ è½½å·²è®­ç»ƒçš„é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨...")

        # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
        yellow_model = YellowFeatureClassifier(
            input_dim=8,
            hidden_dim=config['yellow_classifier']['hidden_dim'],
            num_classes=len(config['class_names']),
            dropout=config['yellow_classifier']['dropout']
        ).to(device)

        yellow_model.load_state_dict(torch.load(yellow_model_path, map_location=device))
        yellow_model.eval()
        print("âœ“ é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨åŠ è½½æˆåŠŸ")

    else:
        print("è®­ç»ƒæ–°çš„é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨...")

        # åˆ›å»ºé»„ç–¸ç‰¹å¾æ•°æ®é›†
        yellow_train_dataset = YellowFeatureDataset(
            train_dir, train_names,
            bilirubin_csv=config['bilirubin_csv'],
            enable_undersample=True,
            undersample_count=200
        )
        yellow_val_dataset = YellowFeatureDataset(
            val_dir, val_names,
            bilirubin_csv=config['bilirubin_csv']
        )

        yellow_train_loader = DataLoader(
            yellow_train_dataset,
            batch_size=config['batch_size'],
            shuffle=True,
            num_workers=config['num_workers']
        )
        yellow_val_loader = DataLoader(
            yellow_val_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=config['num_workers']
        )

        # è®­ç»ƒé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨
        yellow_model = train_yellow_classifier(
            yellow_train_loader, yellow_val_loader, device, config
        )

    # è¯„ä¼°é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨
    print("\n===== é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨è¯„ä¼° =====")
    yellow_test_dataset = YellowFeatureDataset(
        test_dir, test_names,
        bilirubin_csv=config['bilirubin_csv']
    )
    yellow_test_loader = DataLoader(
        yellow_test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers']
    )
    evaluate_yellow_model(yellow_model, yellow_test_loader, device, config)

    # =================== ç¬¬ä¸‰å±‚ï¼šé›†æˆæ‰€æœ‰é¢„æµ‹ ===================
    print("\n" + "=" * 50)
    print("ç¬¬ä¸‰å±‚ï¼šè®­ç»ƒé›†æˆæƒé‡")
    print("=" * 50)

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„é›†æˆæ¨¡å‹
    ensemble_model_path = os.path.join(config['output_dir'], 'ensemble_final.pth')

    if os.path.exists(ensemble_model_path) and config['use_pretrained']:
        print(f"å‘ç°å·²è®­ç»ƒçš„é›†æˆæ¨¡å‹: {ensemble_model_path}")
        print("åŠ è½½å·²è®­ç»ƒçš„é›†æˆæ¨¡å‹...")

        ensemble_model = LayeredEnsemble(
            num_models=5,
            num_classes=len(config['class_names']),
            fusion_method=config['ensemble']['fusion_method']
        ).to(device)

        ensemble_model.load_state_dict(torch.load(ensemble_model_path, map_location=device))
        ensemble_model.eval()
        print("âœ“ é›†æˆæ¨¡å‹åŠ è½½æˆåŠŸ")

    else:
        print("è®­ç»ƒæ–°çš„é›†æˆæ¨¡å‹...")

        # åˆ›å»ºåŒ…å«å›¾åƒå’Œé»„ç–¸ç‰¹å¾çš„å®Œæ•´æ•°æ®é›†ï¼ˆç”¨äºé›†æˆè®­ç»ƒï¼‰
        full_train_dataset = Jaundice3ClassDataset_Full(
            train_dir, train_names,
            bilirubin_csv=config['bilirubin_csv'],
            transform=transforms['convnext'],  # ä½¿ç”¨ç»Ÿä¸€çš„å˜æ¢
            enable_undersample=True,
            undersample_count=200
        )
        full_val_dataset = Jaundice3ClassDataset_Full(
            val_dir, val_names,
            bilirubin_csv=config['bilirubin_csv'],
            transform=transforms['convnext']
        )

        full_train_loader = DataLoader(
            full_train_dataset,
            batch_size=config['batch_size'],
            shuffle=True,
            num_workers=config['num_workers']
        )
        full_val_loader = DataLoader(
            full_val_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=config['num_workers']
        )

        # è®­ç»ƒé›†æˆæƒé‡
        ensemble_model = train_ensemble_weights(
            image_models, yellow_model, full_train_loader, full_val_loader, device, config
        )

    # =================== æœ€ç»ˆè¯„ä¼° ===================
    print("\n" + "=" * 50)
    print("æœ€ç»ˆè¯„ä¼°")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    full_test_dataset = Jaundice3ClassDataset_Full(
        test_dir, test_names,
        bilirubin_csv=config['bilirubin_csv'],
        transform=transforms['convnext']
    )
    full_test_loader = DataLoader(
        full_test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers']
    )

    # è¯„ä¼°æœ€ç»ˆé›†æˆæ¨¡å‹
    print("\n===== æœ€ç»ˆé›†æˆæ¨¡å‹è¯„ä¼° =====")
    final_results = evaluate_ensemble_model(
        image_models, yellow_model, ensemble_model, full_test_loader, device, config
    )

    # ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœæ˜¯æ–°è®­ç»ƒçš„ï¼‰
    if not (os.path.exists(ensemble_model_path) and config['use_pretrained']):
        save_ensemble_model(ensemble_model, config)

    # ç”Ÿæˆç»Ÿä¸€çš„è¯„ä¼°æŠ¥å‘Š
    generate_unified_report(image_models, yellow_model, ensemble_model, final_results, config)

    print("\næ–¹æ¡ˆCåˆ†å±‚é›†æˆè®­ç»ƒå®Œæˆï¼")
    return final_results


def save_ensemble_model(ensemble_model, config):
    """åªä¿å­˜é›†æˆæ¨¡å‹ï¼ˆå›¾åƒæ¨¡å‹å·²ç»å­˜åœ¨ï¼‰"""
    ensemble_path = os.path.join(config['output_dir'], 'ensemble_final.pth')
    torch.save(ensemble_model.state_dict(), ensemble_path)
    print(f"âœ“ é›†æˆæ¨¡å‹å·²ä¿å­˜åˆ°: {ensemble_path}")


def generate_unified_report(image_models, yellow_model, ensemble_model, final_results, config):
    """ç”Ÿæˆç»Ÿä¸€çš„è¯„ä¼°æŠ¥å‘Š"""

    print("\n" + "=" * 60)
    print("æ–¹æ¡ˆCåˆ†å±‚é›†æˆæœ€ç»ˆæŠ¥å‘Š")
    print("=" * 60)

    print(f"\næ¶æ„æ€»ç»“:")
    print(f"- ç¬¬ä¸€å±‚: {len(image_models)} ä¸ªå›¾åƒæ¨¡å‹ (å¤ç”¨å·²è®­ç»ƒæ¨¡å‹)")
    print(f"- ç¬¬äºŒå±‚: 1 ä¸ªé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ (æ–°è®­ç»ƒ)")
    print(f"- ç¬¬ä¸‰å±‚: é›†æˆæ¨¡å‹ (èåˆæ‰€æœ‰é¢„æµ‹)")
    print(f"- èåˆæ–¹æ³•: {config['ensemble']['fusion_method']}")

    print(f"\nå¤ç”¨çš„å›¾åƒæ¨¡å‹:")
    for model_name in image_models.keys():
        if config['reuse_image_models'] and model_name in config['existing_model_paths']:
            print(f"  âœ“ {model_name}: {config['existing_model_paths'][model_name]}")

    print(f"\næœ€ç»ˆæ€§èƒ½:")
    print(f"- å‡†ç¡®ç‡: {final_results['accuracy']:.4f}")
    print(f"- F1åˆ†æ•°: {final_results['f1']:.4f}")

    # å¦‚æœä½¿ç”¨åŠ æƒèåˆï¼Œæ˜¾ç¤ºæƒé‡
    if config['ensemble']['fusion_method'] == 'weighted':
        weights = torch.softmax(ensemble_model.weights, dim=0).cpu().numpy()
        model_names = list(image_models.keys()) + ['YellowFeatures']
        print(f"\næ¨¡å‹æƒé‡åˆ†é…:")
        for name, weight in zip(model_names, weights):
            print(f"  {name}: {weight:.4f}")

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = os.path.join(config['output_dir'], 'layered_ensemble_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("æ–¹æ¡ˆCåˆ†å±‚é›†æˆæœ€ç»ˆæŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"æ¶æ„æ€»ç»“:\n")
        f.write(f"- ç¬¬ä¸€å±‚: {len(image_models)} ä¸ªå›¾åƒæ¨¡å‹ (å¤ç”¨å·²è®­ç»ƒæ¨¡å‹)\n")
        f.write(f"- ç¬¬äºŒå±‚: 1 ä¸ªé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ (æ–°è®­ç»ƒ)\n")
        f.write(f"- ç¬¬ä¸‰å±‚: é›†æˆæ¨¡å‹ (èåˆæ‰€æœ‰é¢„æµ‹)\n")
        f.write(f"- èåˆæ–¹æ³•: {config['ensemble']['fusion_method']}\n\n")

        f.write(f"å¤ç”¨çš„å›¾åƒæ¨¡å‹:\n")
        for model_name in image_models.keys():
            if config['reuse_image_models'] and model_name in config['existing_model_paths']:
                f.write(f"  âœ“ {model_name}: {config['existing_model_paths'][model_name]}\n")

        f.write(f"\næœ€ç»ˆæ€§èƒ½:\n")
        f.write(f"- å‡†ç¡®ç‡: {final_results['accuracy']:.4f}\n")
        f.write(f"- F1åˆ†æ•°: {final_results['f1']:.4f}\n\n")

        if config['ensemble']['fusion_method'] == 'weighted':
            weights = torch.softmax(ensemble_model.weights, dim=0).cpu().numpy()
            model_names = list(image_models.keys()) + ['YellowFeatures']
            f.write(f"æ¨¡å‹æƒé‡åˆ†é…:\n")
            for name, weight in zip(model_names, weights):
                f.write(f"  {name}: {weight:.4f}\n")

    print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


# æ·»åŠ è¾…åŠ©å‡½æ•°
def train_single_model_image_only(model, train_loader, val_loader, device, epochs=10,
                                  lr=1e-4, weight_decay=1e-4, patience=5, model_path=None):
    """è®­ç»ƒåªä½¿ç”¨å›¾åƒçš„å•ä¸ªæ¨¡å‹"""

    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)

    # è·å–ç±»åˆ«æƒé‡
    all_train_labels = []
    for batch in train_loader:
        _, labels, _ = batch
        all_train_labels.extend(labels.numpy())

    class_weights = compute_class_weights(np.array(all_train_labels)).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # æ—©åœ
    best_val_f1 = 0
    no_improve_epochs = 0

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} Training"):
            inputs, labels, _ = batch
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            _, outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            train_correct += predicted.eq(labels).sum().item()
            train_total += labels.size(0)

        train_loss = train_loss / train_total
        train_acc = train_correct / train_total

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        val_preds = []
        val_labels_list = []

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch + 1}/{epochs} Validation"):
                inputs, labels, _ = batch
                inputs, labels = inputs.to(device), labels.to(device)

                _, outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * inputs.size(0)
                _, predicted = outputs.max(1)
                val_correct += predicted.eq(labels).sum().item()
                val_total += labels.size(0)

                val_preds.extend(predicted.cpu().numpy())
                val_labels_list.extend(labels.cpu().numpy())

        val_loss = val_loss / val_total
        val_acc = val_correct / val_total
        val_f1 = f1_score(val_labels_list, val_preds, average='macro')

        # æ›´æ–°è°ƒåº¦å™¨
        scheduler.step(val_f1)

        # æ‰“å°è¿›åº¦
        print(f"Epoch {epoch + 1}/{epochs}")
        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")

        # æ£€æŸ¥æ”¹è¿›
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            no_improve_epochs = 0

            # ä¿å­˜æ¨¡å‹
            if model_path:
                torch.save(model.state_dict(), model_path)
                print(f"Model saved to {model_path}")
        else:
            no_improve_epochs += 1
            print(f"No improvement for {no_improve_epochs} epochs")

        # æ—©åœ
        if no_improve_epochs >= patience:
            print("Early stopping triggered")
            break

    # åŠ è½½æœ€ä½³æ¨¡å‹
    if model_path and os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path))

    return model


def evaluate_model_image_only(model, test_loader, device, class_names=None, roc_save_path=None):
    """è¯„ä¼°åªä½¿ç”¨å›¾åƒçš„æ¨¡å‹"""

    if class_names is None:
        class_names = ['Mild', 'Moderate', 'Severe']

    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            inputs, labels, _ = batch
            inputs = inputs.to(device)
            labels = labels.to(device)

            _, outputs = model(inputs)
            probs = F.softmax(outputs, dim=1)
            preds = torch.argmax(outputs, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro')

    print(f"å›¾åƒæ¨¡å‹æ€§èƒ½:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # æ‰“å°åˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

    # ç»˜åˆ¶ROCæ›²çº¿
    if roc_save_path:
        plot_multiclass_roc(all_labels, all_probs, class_names, save_path=roc_save_path)

    return {
        "accuracy": accuracy,
        "f1": f1,
        "all_labels": all_labels,
        "all_preds": all_preds,
        "all_probs": all_probs
    }


def evaluate_yellow_model(model, test_loader, device, config):
    """è¯„ä¼°é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨"""
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in test_loader:
            yellow_features = batch['yellow_features'].to(device)
            labels = torch.tensor(batch['label']).to(device)

            outputs = model(yellow_features)
            _, predicted = torch.max(outputs, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro')

    print(f"é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨æ€§èƒ½:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # æ‰“å°åˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, target_names=config['class_names']))


def evaluate_ensemble_model(image_models, yellow_model, ensemble_model, test_loader, device, config):
    """è¯„ä¼°æœ€ç»ˆé›†æˆæ¨¡å‹"""

    # è®¾ç½®æ‰€æœ‰æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    for model in image_models.values():
        model.eval()
    yellow_model.eval()
    ensemble_model.eval()

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in test_loader:
            images = batch['image'].to(device)
            yellow_features = batch['yellow_features'].to(device)
            labels = batch['label'].to(device)

            # è·å–æ‰€æœ‰æ¨¡å‹çš„logits
            logits_list = []

            # å›¾åƒæ¨¡å‹é¢„æµ‹
            for model_name, model in image_models.items():
                _, logits = model(images)
                logits_list.append(logits)

            # é»„ç–¸ç‰¹å¾æ¨¡å‹é¢„æµ‹
            yellow_logits = yellow_model(yellow_features)
            logits_list.append(yellow_logits)

            # é›†æˆé¢„æµ‹
            ensemble_logits = ensemble_model(logits_list)
            probabilities = torch.softmax(ensemble_logits, dim=1)
            _, predicted = torch.max(ensemble_logits, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probabilities.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro')

    print(f"æœ€ç»ˆé›†æˆæ¨¡å‹æ€§èƒ½:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # æ‰“å°è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\næœ€ç»ˆåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, target_names=config['class_names']))

    # è®¡ç®—AUC
    try:
        auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')
        print(f"ROC AUC: {auc:.4f}")
    except:
        print("æ— æ³•è®¡ç®—AUC")

    # æ‰“å°æ¨¡å‹æƒé‡ï¼ˆå¦‚æœä½¿ç”¨åŠ æƒèåˆï¼‰
    if config['ensemble']['fusion_method'] == 'weighted':
        weights = torch.softmax(ensemble_model.weights, dim=0).cpu().numpy()
        model_names = list(image_models.keys()) + ['YellowFeatures']
        print(f"\næ¨¡å‹æƒé‡:")
        for name, weight in zip(model_names, weights):
            print(f"{name}: {weight:.4f}")

    return {
        'accuracy': accuracy,
        'f1': f1,
        'all_labels': all_labels,
        'all_preds': all_preds,
        'all_probs': all_probs
    }


# æ·»åŠ å®Œæ•´æ•°æ®é›†ç±»ï¼ˆåŒ…å«å›¾åƒå’Œé»„ç–¸ç‰¹å¾ï¼‰
class Jaundice3ClassDataset_Full(Dataset):
    """åŒ…å«å›¾åƒå’Œé»„ç–¸ç‰¹å¾çš„å®Œæ•´æ•°æ®é›†ç±»ï¼ˆç”¨äºé›†æˆè®­ç»ƒï¼‰"""

    def __init__(self, root_dir, patient_names, split='train',
                 bilirubin_csv=None, transform=None,
                 enable_undersample=False, undersample_count=None):
        self.root_dir = root_dir
        self.split = split
        self.transform = transform
        self.label_names = ['mild', 'moderate', 'severe']
        self.class_to_idx = {name: i for i, name in enumerate(self.label_names)}

        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        self.ffe = FacialFeatureExtractor(use_mediapipe=True)

        self.samples = []

        # è¯»å–èƒ†çº¢ç´ æ•°æ®
        assert bilirubin_csv is not None, "Must provide bilirubin csv file path"
        df = pd.read_excel(bilirubin_csv)
        df.columns = df.columns.str.strip()
        self.serial2bil = dict(zip(df['åºå·'], df['26ã€æ€»èƒ†çº¢ç´ å€¼ï¼ˆumol/Lï¼‰']))

        for folder_name in patient_names:
            serial = extract_serial_number(folder_name)
            if serial is None or serial not in self.serial2bil:
                print(f"[Warning] {folder_name} not matched to table serial number, skipping")
                continue

            bil = float(self.serial2bil[serial])
            # ç›´æ¥ä¸‰åˆ†ç±»
            if bil < 171:
                label_name = 'mild'
            elif bil < 342:
                label_name = 'moderate'
            else:
                label_name = 'severe'
            label = self.class_to_idx[label_name]

            patient_folder = os.path.join(self.root_dir, folder_name)
            if not os.path.isdir(patient_folder):
                continue

            imgs = [f for f in os.listdir(patient_folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
            if not imgs:
                print(f"[Warning] No images in {patient_folder}, skipping")
                continue

            for img_name in imgs:
                img_path = os.path.join(patient_folder, img_name)
                self.samples.append((img_path, label, serial))

        # ä¸‹é‡‡æ ·å¤„ç†
        if self.split == 'train' and enable_undersample:
            print("Applying undersampling to full dataset...")
            label_to_samples = defaultdict(list)
            for sample in self.samples:
                label_to_samples[sample[1]].append(sample)
            if undersample_count is None:
                undersample_count = min(len(s) for s in label_to_samples.values())
            print(f"Undersample count per class: {undersample_count}")
            new_samples = []
            for label, group in label_to_samples.items():
                if len(group) >= undersample_count:
                    new_samples.extend(random.sample(group, undersample_count))
                else:
                    new_samples.extend(group)
            self.samples = new_samples
            print(f"After undersampling: {Counter([s[1] for s in self.samples])}")

        print(f"Loaded {len(self.samples)} full samples. Class distribution:")
        print(Counter([s[1] for s in self.samples]))

    def extract_yellow_features(self, image):
        """ä»PILå›¾åƒä¸­æå–é»„ç–¸ç‰¹å¾"""
        try:
            # æå–çœ¼éƒ¨åŒºåŸŸ
            left_eye, right_eye = self.ffe.extract_sclera_patches(image)

            # ç¡®ä¿åŒºåŸŸæå–æˆåŠŸ
            if left_eye is None or right_eye is None:
                # å›é€€åˆ°ç›´æ¥è£å‰ª
                w, h = image.size
                left_eye = image.crop((0, 0, w // 4, h // 4)) if left_eye is None else left_eye
                right_eye = image.crop((3 * w // 4, 0, w, h // 4)) if right_eye is None else right_eye

            # æå–é»„ç–¸æŒ‡æ ‡
            left_eye_yellow = extract_yellow_metrics(left_eye)
            right_eye_yellow = extract_yellow_metrics(right_eye)

            # åˆå¹¶ç‰¹å¾ (æ¯ä¸ªçœ¼éƒ¨4ä¸ªç‰¹å¾ï¼Œæ€»å…±8ä¸ª)
            features = np.concatenate([left_eye_yellow, right_eye_yellow])
            return features

        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            return np.zeros(8)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        try:
            img_path, label, serial = self.samples[idx]
            img = Image.open(img_path).convert('RGB')

            # Apply data augmentation for training
            if self.split == 'train':
                np_img = np.array(img)
                np_img = color_sensitive_augmentation(np_img)
                img = Image.fromarray(np_img)

            # æå–é»„ç–¸ç‰¹å¾ï¼ˆåœ¨å˜æ¢ä¹‹å‰ï¼‰
            yellow_features = self.extract_yellow_features(img)

            # Apply transform if provided
            if self.transform:
                img_tensor = self.transform(img)
            else:
                # Default transform
                transform = T.Compose([
                    T.Resize((224, 224)),
                    T.ToTensor(),
                    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
                ])
                img_tensor = transform(img)

            return {
                'image': img_tensor,
                'yellow_features': torch.FloatTensor(yellow_features),
                'label': torch.tensor(label),
                'serial': serial
            }

        except Exception as e:
            print(f"Cannot process sample {idx}, error: {e}")
            return self.__getitem__(max(0, idx - 1))


def save_all_models(image_models, yellow_model, ensemble_model, config):
    """ä¿å­˜æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹"""

    # ä¿å­˜å›¾åƒæ¨¡å‹
    for model_name, model in image_models.items():
        model_path = os.path.join(config['output_dir'], f"{model_name.lower()}_final.pth")
        torch.save(model.state_dict(), model_path)
        print(f"Saved {model_name} to {model_path}")

    # ä¿å­˜é»„ç–¸ç‰¹å¾æ¨¡å‹
    yellow_path = os.path.join(config['output_dir'], 'yellow_classifier_final.pth')
    torch.save(yellow_model.state_dict(), yellow_path)
    print(f"Saved yellow classifier to {yellow_path}")

    # ä¿å­˜é›†æˆæ¨¡å‹
    ensemble_path = os.path.join(config['output_dir'], 'ensemble_final.pth')
    torch.save(ensemble_model.state_dict(), ensemble_path)
    print(f"Saved ensemble model to {ensemble_path}")


def generate_unified_report(image_models, yellow_model, ensemble_model, final_results, config):
    """ç”Ÿæˆç»Ÿä¸€çš„è¯„ä¼°æŠ¥å‘Š"""

    print("\n" + "=" * 60)
    print("æ–¹æ¡ˆCåˆ†å±‚é›†æˆæœ€ç»ˆæŠ¥å‘Š")
    print("=" * 60)

    print(f"\næ¶æ„æ€»ç»“:")
    print(f"- ç¬¬ä¸€å±‚: {len(image_models)} ä¸ªå›¾åƒæ¨¡å‹ (åªä½¿ç”¨å›¾åƒ)")
    print(f"- ç¬¬äºŒå±‚: 1 ä¸ªé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ (åªä½¿ç”¨é»„ç–¸ç‰¹å¾)")
    print(f"- ç¬¬ä¸‰å±‚: é›†æˆæ¨¡å‹ (èåˆæ‰€æœ‰é¢„æµ‹)")
    print(f"- èåˆæ–¹æ³•: {config['ensemble']['fusion_method']}")

    print(f"\næœ€ç»ˆæ€§èƒ½:")
    print(f"- å‡†ç¡®ç‡: {final_results['accuracy']:.4f}")
    print(f"- F1åˆ†æ•°: {final_results['f1']:.4f}")

    # å¦‚æœä½¿ç”¨åŠ æƒèåˆï¼Œæ˜¾ç¤ºæƒé‡
    if config['ensemble']['fusion_method'] == 'weighted':
        weights = torch.softmax(ensemble_model.weights, dim=0).cpu().numpy()
        model_names = list(image_models.keys()) + ['YellowFeatures']
        print(f"\næ¨¡å‹æƒé‡åˆ†é…:")
        for name, weight in zip(model_names, weights):
            print(f"  {name}: {weight:.4f}")

    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"- å›¾åƒæ¨¡å‹è®­ç»ƒè½®æ•°: {config['backbone_epochs']}")
    print(f"- é»„ç–¸åˆ†ç±»å™¨è®­ç»ƒè½®æ•°: {config['yellow_classifier']['epochs']}")
    print(f"- é›†æˆæƒé‡è®­ç»ƒè½®æ•°: {config['ensemble']['weights_epochs']}")
    print(f"- æ‰¹æ¬¡å¤§å°: {config['batch_size']}")

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = os.path.join(config['output_dir'], 'layered_ensemble_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("æ–¹æ¡ˆCåˆ†å±‚é›†æˆæœ€ç»ˆæŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"æ¶æ„æ€»ç»“:\n")
        f.write(f"- ç¬¬ä¸€å±‚: {len(image_models)} ä¸ªå›¾åƒæ¨¡å‹ (åªä½¿ç”¨å›¾åƒ)\n")
        f.write(f"- ç¬¬äºŒå±‚: 1 ä¸ªé»„ç–¸ç‰¹å¾åˆ†ç±»å™¨ (åªä½¿ç”¨é»„ç–¸ç‰¹å¾)\n")
        f.write(f"- ç¬¬ä¸‰å±‚: é›†æˆæ¨¡å‹ (èåˆæ‰€æœ‰é¢„æµ‹)\n")
        f.write(f"- èåˆæ–¹æ³•: {config['ensemble']['fusion_method']}\n\n")
        f.write(f"æœ€ç»ˆæ€§èƒ½:\n")
        f.write(f"- å‡†ç¡®ç‡: {final_results['accuracy']:.4f}\n")
        f.write(f"- F1åˆ†æ•°: {final_results['f1']:.4f}\n\n")

        if config['ensemble']['fusion_method'] == 'weighted':
            weights = torch.softmax(ensemble_model.weights, dim=0).cpu().numpy()
            model_names = list(image_models.keys()) + ['YellowFeatures']
            f.write(f"æ¨¡å‹æƒé‡åˆ†é…:\n")
            for name, weight in zip(model_names, weights):
                f.write(f"  {name}: {weight:.4f}\n")

    print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")



# æ›´æ–°é…ç½®
config = {
    'seed': 42,
    'dataset_dir': r"C:\Users\MichaelY\Desktop\jaundice_dataset\arcface_dataset",
    'bilirubin_csv': r"C:\Users\MichaelY\Documents\WeChat Files\wxid_sckyac3nu7h521\FileStorage\File\2025-05\213008980_æŒ‰åºå·_è‚ç‚æ‚£è€…è¡¥å……é—®å·-åŒ»æŠ¤_313_313.xlsx",
    'output_dir': r"C:\Users\MichaelY\Desktop\jaundice3cls",
    'models': ['ConvNext', 'Swin', 'EfficientNet', 'ViT','YOLO'],
    'features': ['yellow'],
    'batch_size': 8,
    'num_workers': 0,
    'freeze_base': True,
    'use_pretrained': True,

    # å¤ç”¨å·²æœ‰çš„å›¾åƒæ¨¡å‹
    'reuse_image_models': True,
    'existing_model_paths': {
        'ConvNext': r"C:\Users\MichaelY\Desktop\jaundice3cls\convnext_best.pth",
        'Swin': r"C:\Users\MichaelY\Desktop\jaundice3cls\swin_best.pth",
        'EfficientNet': r"C:\Users\MichaelY\Desktop\jaundice3cls\efficientnet_best.pth",
        'ViT': r"C:\Users\MichaelY\Desktop\jaundice3cls\vit_best.pth",
        'YOLO': r"C:\Users\MichaelY\Desktop\jaundice3cls\yolo_best.pth"
    },

    'yolo': {
        'model_path': r"E:\yolo11m-cls.pt",
        'img_size': 224,
        'lr': 0.001,
        'weight_decay': 1e-4,
        'epochs': 50
    },

    # é»„ç–¸ç‰¹å¾åˆ†ç±»å™¨é…ç½®
    'yellow_classifier': {
        'hidden_dim': 64,
        'dropout': 0.3,
        'epochs': 100,
        'lr': 0.001,
        'weight_decay': 1e-4
    },

    # é›†æˆé…ç½® - ä½¿ç”¨Self Attentionèåˆ
    'ensemble': {
        'fusion_method': 'dynamic_weight',  # ä½¿ç”¨çœŸæ­£çš„Self Attention
        'lr': 0.001,
        'weight_decay': 1e-4,
        'epochs': 100,
        'patience': 15,  # æ—©åœè€å¿ƒå€¼
        'min_delta': 0.001,  # æ—©åœæœ€å°æ”¹è¿›
        'hidden_dim': 64
    },

    'class_names': ['Mild', 'Moderate', 'Severe']
}


def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–å’Œæ–‡ä»¶"""

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing_models = []
    for model_name, path in config['existing_model_paths'].items():
        if model_name != 'YOLO' and not os.path.exists(path):  # YOLOå¯ä»¥ä½¿ç”¨é»˜è®¤æƒé‡
            missing_models.append(f"{model_name}: {path}")

    if missing_models:
        print("âŒ ç¼ºå¤±ä»¥ä¸‹æ¨¡å‹æ–‡ä»¶:")
        for missing in missing_models:
            print(f"  - {missing}")
        print("æ³¨æ„ï¼šYOLOæ¨¡å‹å¦‚æœä¸å­˜åœ¨ä¼šä½¿ç”¨é»˜è®¤é¢„è®­ç»ƒæƒé‡")
        return False

    # æ£€æŸ¥æ•°æ®é›†ç›®å½•
    if not os.path.exists(config['dataset_dir']):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {config['dataset_dir']}")
        return False

    # æ£€æŸ¥èƒ†çº¢ç´ CSVæ–‡ä»¶
    if not os.path.exists(config['bilirubin_csv']):
        print(f"âŒ èƒ†çº¢ç´ CSVæ–‡ä»¶ä¸å­˜åœ¨: {config['bilirubin_csv']}")
        return False

    print("âœ… æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡")
    return True


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("è¯·å…ˆå‡†å¤‡å¥½æ‰€éœ€çš„æ¨¡å‹æ–‡ä»¶å’Œæ•°æ®")
        exit(1)

    print("ğŸš€ å¼€å§‹Dynamic Weight Fusionå®Œæ•´è®­ç»ƒå’Œè¯„ä¼°...")

    try:
        # ä½¿ç”¨å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
        unified_results, bootstrap_results = train_and_evaluate_dynamic_fusion_comprehensive(config)

        print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()

